{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f816cb-6abb-4cf9-9a0b-d1382a4c9910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "concat_path = \"XTT22_train.fa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a015b5-fad5-4d11-adcd-2fb18c594536",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_fasta_path = os.path.abspath(concat_path)\n",
    "output_dir = os.path.abspath(\"preprocessed_data\")\n",
    "output_yaml = f\"\"\"\n",
    "- datapaths: [\"{full_fasta_path}\"]\n",
    "  output_dir: \"{output_dir}\"\n",
    "  output_prefix: XTT22_train\n",
    "  train_split: 0.9\n",
    "  valid_split: 0.05\n",
    "  test_split: 0.05\n",
    "  overwrite: True\n",
    "  embed_reverse_complement: true\n",
    "  random_reverse_complement: 0.0\n",
    "  random_lineage_dropout: 0.0\n",
    "  include_sequence_id: false\n",
    "  transcribe: \"back_transcribe\"\n",
    "  force_uppercase: false\n",
    "  indexed_dataset_dtype: \"uint8\"\n",
    "  tokenizer_type: \"Byte-Level\"\n",
    "  vocab_file: null\n",
    "  vocab_size: null\n",
    "  merges_file: null\n",
    "  pretrained_tokenizer_model: null\n",
    "  special_tokens: null\n",
    "  fast_hf_tokenizer: true\n",
    "  append_eod: true\n",
    "  enforce_sample_length: null\n",
    "  ftfy: false\n",
    "  workers: 1\n",
    "  preproc_concurrency: 100000\n",
    "  chunksize: 25\n",
    "  drop_empty_sequences: true\n",
    "  nnn_filter: false  # If you split your fasta on NNN (in human these are contigs), then you should set this to true.\n",
    "  seed: 12342  # Not relevant because we are not using random reverse complement or lineage dropout.\n",
    "\"\"\"\n",
    "with open(\"preprocess_config.yaml\", \"w\") as f:\n",
    "    print(output_yaml, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aadbf5cb-03b5-4a20-96e5-7bb0a7e5b942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-24 12:37:06 nemo_logging:393] Using byte-level tokenization\n",
      "[NeMo I 2025-05-24 12:37:06 nemo_logging:393] Created temporary binary datasets: /workspace/preprocessed_data/XTT22_train_byte-level_train.bin.tmp /workspace/preprocessed_data/XTT22_train_byte-level_val.bin.tmp /workspace/preprocessed_data/XTT22_train_byte-level_test.bin.tmp\n",
      "[NeMo I 2025-05-24 13:12:11 nemo_logging:393] Average preprocessing time per sequence: 0.04470627161196968\n",
      "[NeMo I 2025-05-24 13:12:11 nemo_logging:393] Average indexing time per sequence: 0.1463382052460373\n",
      "[NeMo I 2025-05-24 13:12:11 nemo_logging:393] Number of sequences processed: 12092\n",
      "[NeMo I 2025-05-24 13:12:11 nemo_logging:393] Finished preprocessing XTT22_train ([PosixPath('/workspace/XTT22_train.fa')]) in 2105.082 seconds with 1 workers.\n"
     ]
    }
   ],
   "source": [
    "!preprocess_evo2 --config preprocess_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3df5d34a-d5fe-4fa9-ac3d-65b9696a9045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14G\n",
      "-rw-r--r-- 1 root root 936M May 24 13:11 XTT22_train_byte-level_test.bin\n",
      "-rw-r--r-- 1 root root  12K May 24 13:12 XTT22_train_byte-level_test.idx\n",
      "-rw-r--r-- 1 root root  13G May 24 13:12 XTT22_train_byte-level_train.bin\n",
      "-rw-r--r-- 1 root root 213K May 24 13:12 XTT22_train_byte-level_train.idx\n",
      "-rw-r--r-- 1 root root 411M May 24 13:12 XTT22_train_byte-level_val.bin\n",
      "-rw-r--r-- 1 root root  12K May 24 13:12 XTT22_train_byte-level_val.idx\n"
     ]
    }
   ],
   "source": [
    "!ls -lh preprocessed_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec85fb63-1214-45f4-849b-87f54166d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /workspace/hyena_modified.py /usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/model/hyena.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a20f4f4-8c19-44b7-94b5-db53eff61976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda129.so')\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Using byte-level tokenization\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2025-06-02 07:46:00 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "    \n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo W 2025-06-02 07:46:00 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py:1090: `trainer.init_module` cannot fully support proper instantiation of your model with the `MegatronStrategy` strategy. Please instantiate your model inside the`LightningModule.configure_model` hook instead\n",
      "    \n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Padded vocab_size: 512, original vocab_size: 512, dummy tokens: 0.\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.2.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.6.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.9.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.13.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.16.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.20.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.23.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.27.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.30.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:05 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n",
      "[NeMo I 2025-06-02 07:46:05 nemo_logging:393] Using FullyParallelSaveStrategyWrapper(torch_dist, 1) dist-ckpt save strategy.\n",
      "Overwriting old incomplete / corrupted checkpoint...\n",
      "[NeMo I 2025-06-02 07:46:18 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 0 : Start time: 1748850365.187s : Save duration: 12.931s\n",
      "[NeMo I 2025-06-02 07:46:18 nemo_logging:393] Converted Hyena model to Nemo, model saved to nemo2_evo2_7b\n"
     ]
    }
   ],
   "source": [
    "!evo2_convert_to_nemo2 \\\n",
    "  --model-path /workspace/savanna_evo2_7b/savanna_evo2_7b.pt \\\n",
    "  --model-size 7b --output-dir nemo2_evo2_7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adacfca1-de37-4ece-919b-c720dc285ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ è®¾ç½®PyTorchåˆ†å¸ƒå¼è¶…æ—¶: 7200ç§’\n",
      "Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda129.so')\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Using byte-level tokenization\n",
      "å¯ç”¨ LoRA å¾®è°ƒ...\n",
      "æ¨¡å‹ç»“æ„è°ƒè¯•ä¿¡æ¯:\n",
      "--------------------------------------------------\n",
      "æ¨¡å‹æ€»å…±æœ‰ 1 ä¸ªæ¨¡å—\n",
      "å°è¯•è®¿é—®æ¨¡å‹çš„å…¶ä»–å±æ€§...\n",
      "æ¨¡å—ç»“æ„:\n",
      "   1.  (HyenaModel)\n",
      "\n",
      "ç›®æ ‡æ¨¡å¼: ['module.decoder.layers.17.self_attention.linear_qkv']\n",
      "\n",
      "æ‰€æœ‰çº¿æ€§å±‚ (0):\n",
      "\n",
      "æ‰¾åˆ°çš„æ³¨æ„åŠ›ç›¸å…³å±‚ (0):\n",
      "\n",
      "æ‰¾åˆ°çš„çº¿æ€§å±‚ (0):\n",
      "\n",
      "æ‰¾åˆ°çš„QKVå±‚ (0):\n",
      "âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•çº¿æ€§å±‚ï¼æ¨¡å‹å¯èƒ½è¿˜æ²¡æœ‰å®Œå…¨åˆå§‹åŒ–ã€‚\n",
      "è¿™åœ¨ NeMo/Megatron æ¡†æ¶ä¸­æ˜¯æ­£å¸¸çš„ï¼Œæ¨¡å‹ç»“æ„ä¼šåœ¨è®­ç»ƒå¼€å§‹æ—¶åˆå§‹åŒ–ã€‚\n",
      "LoRA é…ç½®å·²ä¿å­˜ï¼Œå°†åœ¨æ¨¡å‹å®Œå…¨åˆå§‹åŒ–ååº”ç”¨ã€‚\n",
      "æ€»å‚æ•°æ•°é‡: 0\n",
      "å¯è®­ç»ƒå‚æ•°æ•°é‡: 0\n",
      "âš ï¸  è­¦å‘Š: æ¨¡å‹æ€»å‚æ•°æ•°é‡ä¸º0ï¼Œå¯èƒ½æ¨¡å‹åˆå§‹åŒ–æœ‰é—®é¢˜\n",
      "âš ï¸  è­¦å‘Š: æ²¡æœ‰å¯è®­ç»ƒçš„å‚æ•°ï¼ŒLoRA å¯èƒ½æ²¡æœ‰æ­£ç¡®åº”ç”¨\n",
      "LoRA å°†åœ¨è®­ç»ƒå¼€å§‹æ—¶è‡ªåŠ¨åº”ç”¨\n",
      "å·²æ·»åŠ  LoRA å›è°ƒï¼Œå°†åœ¨è®­ç»ƒå¼€å§‹æ—¶åº”ç”¨ LoRA\n",
      "[NeMo W 2025-06-05 09:21:58 nemo_logging:405] WandB is currently turned off.\n",
      "[NeMo W 2025-06-05 09:21:58 nemo_logging:405] User-set tensorboard is currently turned off. Internally one may still be set by NeMo2.\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Experiments will be logged at results/evo2/dev\n",
      "[NeMo W 2025-06-05 09:21:58 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :results/evo2/checkpoints. Training from scratch.\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has data parallel group : [0, 1]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0, 1]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1]]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] All context parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] All model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] All tensor model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] All embedding group ranks: [[0], [1]]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "ğŸ”§ è®¾ç½®PyTorchåˆ†å¸ƒå¼è¶…æ—¶: 7200ç§’\n",
      "Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda129.so')\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "å¯ç”¨ LoRA å¾®è°ƒ...\n",
      "æ¨¡å‹ç»“æ„è°ƒè¯•ä¿¡æ¯:\n",
      "--------------------------------------------------\n",
      "æ¨¡å‹æ€»å…±æœ‰ 1 ä¸ªæ¨¡å—\n",
      "å°è¯•è®¿é—®æ¨¡å‹çš„å…¶ä»–å±æ€§...\n",
      "æ¨¡å—ç»“æ„:\n",
      "   1.  (HyenaModel)\n",
      "\n",
      "ç›®æ ‡æ¨¡å¼: ['module.decoder.layers.17.self_attention.linear_qkv']\n",
      "\n",
      "æ‰€æœ‰çº¿æ€§å±‚ (0):\n",
      "\n",
      "æ‰¾åˆ°çš„æ³¨æ„åŠ›ç›¸å…³å±‚ (0):\n",
      "\n",
      "æ‰¾åˆ°çš„çº¿æ€§å±‚ (0):\n",
      "\n",
      "æ‰¾åˆ°çš„QKVå±‚ (0):\n",
      "âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•çº¿æ€§å±‚ï¼æ¨¡å‹å¯èƒ½è¿˜æ²¡æœ‰å®Œå…¨åˆå§‹åŒ–ã€‚\n",
      "è¿™åœ¨ NeMo/Megatron æ¡†æ¶ä¸­æ˜¯æ­£å¸¸çš„ï¼Œæ¨¡å‹ç»“æ„ä¼šåœ¨è®­ç»ƒå¼€å§‹æ—¶åˆå§‹åŒ–ã€‚\n",
      "LoRA é…ç½®å·²ä¿å­˜ï¼Œå°†åœ¨æ¨¡å‹å®Œå…¨åˆå§‹åŒ–ååº”ç”¨ã€‚\n",
      "æ€»å‚æ•°æ•°é‡: 0\n",
      "å¯è®­ç»ƒå‚æ•°æ•°é‡: 0\n",
      "âš ï¸  è­¦å‘Š: æ¨¡å‹æ€»å‚æ•°æ•°é‡ä¸º0ï¼Œå¯èƒ½æ¨¡å‹åˆå§‹åŒ–æœ‰é—®é¢˜\n",
      "âš ï¸  è­¦å‘Š: æ²¡æœ‰å¯è®­ç»ƒçš„å‚æ•°ï¼ŒLoRA å¯èƒ½æ²¡æœ‰æ­£ç¡®åº”ç”¨\n",
      "LoRA å°†åœ¨è®­ç»ƒå¼€å§‹æ—¶è‡ªåŠ¨åº”ç”¨\n",
      "å·²æ·»åŠ  LoRA å›è°ƒï¼Œå°†åœ¨è®­ç»ƒå¼€å§‹æ—¶åº”ç”¨ LoRA\n",
      "[W605 09:22:13.707703116 Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n",
      "[W605 09:22:13.707728252 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "[W605 09:22:13.715988247 Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n",
      "[W605 09:22:13.716068477 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "ubuntu:328:328 [0] NCCL INFO Bootstrap: Using eth0:172.21.72.20<0>\n",
      "ubuntu:328:328 [0] NCCL INFO cudaDriverVersion 12090\n",
      "ubuntu:328:328 [0] NCCL INFO NCCL version 2.26.3+cuda12.9\n",
      "ubuntu:328:328 [0] NCCL INFO Comm config Blocking set to 1\n",
      "ubuntu:636:636 [1] NCCL INFO cudaDriverVersion 12090\n",
      "ubuntu:636:636 [1] NCCL INFO Bootstrap: Using eth0:172.21.72.20<0>\n",
      "ubuntu:636:636 [1] NCCL INFO NCCL version 2.26.3+cuda12.9\n",
      "ubuntu:636:636 [1] NCCL INFO Comm config Blocking set to 1\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v10 symbol.\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v9 (v9)\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/Plugin: Loaded collnet plugin SHARP (v9)\n",
      "ubuntu:328:1070 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\n",
      "ubuntu:328:1070 [0] NCCL INFO P2P plugin v9 IBext_v9\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/IB : No device found.\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/IB : No device found.\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/Socket : Using [0]eth0:172.21.72.20<0> [1]eth1:172.25.58.75<0> [2]nodelocaldns:169.254.25.10<0> [3]vxlan.calico:10.233.112.0<0> [4]cali829ebf906e6:fe80::ecee:eeff:feee:eeee%cali829ebf906e6<0> [5]cali9ffd33e0038:fe80::ecee:eeff:feee:eeee%cali9ffd33e0038<0> [6]cali210c644397a:fe80::ecee:eeff:feee:eeee%cali210c644397a<0> [7]caliafd05346b22:fe80::ecee:eeff:feee:eeee%caliafd05346b22<0> [8]calicd149c44f0f:fe80::ecee:eeff:feee:eeee%calicd149c44f0f<0> [9]cali62597977d84:fe80::ecee:eeff:feee:eeee%cali62597977d84<0> [10]calidee47cce1bb:fe80::ecee:eeff:feee:eeee%calidee47cce1bb<0> [11]cali2ad9843228b:fe80::ecee:eeff:feee:eeee%cali2ad9843228b<0> [12]calicb98c4ca0f1:fe80::ecee:eeff:feee:eeee%calicb98c4ca0f1<0> [13]cali9c537539f6f:fe80::ecee:eeff:feee:eeee%cali9c537539f6f<0> [14]cali3dd684f6336:fe80::ecee:eeff:feee:eeee%cali3dd684f6336<0> [15]calid294f721236:fe80::ecee:eeff:feee:eeee%calid294f721236<0>\n",
      "ubuntu:328:1070 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. \n",
      "ubuntu:328:1070 [0] NCCL INFO Using network Socket\n",
      "ubuntu:328:1070 [0] NCCL INFO ncclCommInitRankConfig comm 0x37a3d210 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 27000 commId 0x56b91acfc7a7dbab - Init START\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v10 symbol.\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v9 (v9)\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/Plugin: Loaded collnet plugin SHARP (v9)\n",
      "ubuntu:636:1071 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\n",
      "ubuntu:636:1071 [1] NCCL INFO P2P plugin v9 IBext_v9\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/IB : No device found.\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/IB : No device found.\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/Socket : Using [0]eth0:172.21.72.20<0> [1]eth1:172.25.58.75<0> [2]nodelocaldns:169.254.25.10<0> [3]vxlan.calico:10.233.112.0<0> [4]cali829ebf906e6:fe80::ecee:eeff:feee:eeee%cali829ebf906e6<0> [5]cali9ffd33e0038:fe80::ecee:eeff:feee:eeee%cali9ffd33e0038<0> [6]cali210c644397a:fe80::ecee:eeff:feee:eeee%cali210c644397a<0> [7]caliafd05346b22:fe80::ecee:eeff:feee:eeee%caliafd05346b22<0> [8]calicd149c44f0f:fe80::ecee:eeff:feee:eeee%calicd149c44f0f<0> [9]cali62597977d84:fe80::ecee:eeff:feee:eeee%cali62597977d84<0> [10]calidee47cce1bb:fe80::ecee:eeff:feee:eeee%calidee47cce1bb<0> [11]cali2ad9843228b:fe80::ecee:eeff:feee:eeee%cali2ad9843228b<0> [12]calicb98c4ca0f1:fe80::ecee:eeff:feee:eeee%calicb98c4ca0f1<0> [13]cali9c537539f6f:fe80::ecee:eeff:feee:eeee%cali9c537539f6f<0> [14]cali3dd684f6336:fe80::ecee:eeff:feee:eeee%cali3dd684f6336<0> [15]calid294f721236:fe80::ecee:eeff:feee:eeee%calid294f721236<0>\n",
      "ubuntu:636:1071 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. \n",
      "ubuntu:636:1071 [1] NCCL INFO Using network Socket\n",
      "ubuntu:636:1071 [1] NCCL INFO ncclCommInitRankConfig comm 0x3da8e1d0 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 2a000 commId 0x56b91acfc7a7dbab - Init START\n",
      "ubuntu:636:1071 [1] NCCL INFO RAS client listening socket at 127.0.0.1<28028>\n",
      "ubuntu:328:1070 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>\n",
      "ubuntu:636:1071 [1] NCCL INFO Bootstrap timings total 0.001530 (create 0.000040, send 0.000209, recv 0.000707, ring 0.000204, delay 0.000001)\n",
      "ubuntu:328:1070 [0] NCCL INFO Bootstrap timings total 0.543683 (create 0.000045, send 0.016664, recv 0.526136, ring 0.000074, delay 0.000001)\n",
      "ubuntu:636:1071 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff\n",
      "ubuntu:328:1070 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff\n",
      "ubuntu:636:1071 [1] NCCL INFO comm 0x3da8e1d0 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0\n",
      "ubuntu:328:1070 [0] NCCL INFO comm 0x37a3d210 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 00/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 01/24 : 0 1\n",
      "ubuntu:636:1071 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] 0/-1/-1->1->-1 [7] 0/-1/-1->1->-1 [8] 0/-1/-1->1->-1 [9] 0/-1/-1->1->-1 [10] 0/-1/-1->1->-1 [11] 0/-1/-1->1->-1 [12] -1/-1/-1->1->0 [13] -1/-1/-1->1->0 [14] -1/-1/-1->1->0 [15] -1/-1/-1->1->0 [16] -1/-1/-1->1->0 [17] -1/-1/-1->1->0 [18] 0/-1/-1->1->-1 [19] 0/-1/-1->1->-1 [20] 0/-1/-1->1->-1 [21] 0/-1/-1->1->-1 [22] 0/-1/-1->1->-1 [23] 0/-1/-1->1->-1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 02/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 03/24 : 0 1\n",
      "ubuntu:636:1071 [1] NCCL INFO NCCL_BUFFSIZE set by environment to 8388608.\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 04/24 : 0 1\n",
      "ubuntu:636:1071 [1] NCCL INFO P2P Chunksize set to 524288\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 05/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 06/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 07/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 08/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 09/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 10/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 11/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 12/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 13/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 14/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 15/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 16/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 17/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 18/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 19/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 20/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 21/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 22/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 23/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] -1/-1/-1->0->1 [9] -1/-1/-1->0->1 [10] -1/-1/-1->0->1 [11] -1/-1/-1->0->1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] -1/-1/-1->0->1 [19] -1/-1/-1->0->1 [20] -1/-1/-1->0->1 [21] -1/-1/-1->0->1 [22] -1/-1/-1->0->1 [23] -1/-1/-1->0->1\n",
      "ubuntu:328:1070 [0] NCCL INFO NCCL_BUFFSIZE set by environment to 8388608.\n",
      "ubuntu:328:1070 [0] NCCL INFO P2P Chunksize set to 524288\n",
      "ubuntu:328:1070 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 1 directMode 0\n",
      "ubuntu:328:1075 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 92\n",
      "ubuntu:328:1074 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12\n",
      "ubuntu:328:1070 [0] NCCL INFO NCCL_NTHREADS set by environment to 32.\n",
      "\n",
      "[2025-06-05 09:22:14] ubuntu:328:1070 [0] graph/tuning.cc:25 NCCL WARN Invalid NCCL_NTHREADS 32 (minimum 64).\n",
      "\n",
      "[2025-06-05 09:22:14] ubuntu:328:1070 [0] graph/tuning.cc:25 NCCL WARN Invalid NCCL_NTHREADS 32 (minimum 64).\n",
      "\n",
      "[2025-06-05 09:22:14] ubuntu:328:1070 [0] graph/tuning.cc:25 NCCL WARN Invalid NCCL_NTHREADS 32 (minimum 64).\n",
      "ubuntu:328:1070 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "ubuntu:328:1070 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\n",
      "ubuntu:328:1070 [0] NCCL INFO CC Off, workFifoBytes 1048576\n",
      "ubuntu:636:1076 [1] NCCL INFO [Proxy Service] Device 1 CPU core 8\n",
      "ubuntu:636:1077 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 74\n",
      "ubuntu:636:1071 [1] NCCL INFO NCCL_NTHREADS set by environment to 32.\n",
      "\n",
      "[2025-06-05 09:22:14] ubuntu:636:1071 [1] graph/tuning.cc:25 NCCL WARN Invalid NCCL_NTHREADS 32 (minimum 64).\n",
      "\n",
      "[2025-06-05 09:22:14] ubuntu:636:1071 [1] graph/tuning.cc:25 NCCL WARN Invalid NCCL_NTHREADS 32 (minimum 64).\n",
      "\n",
      "[2025-06-05 09:22:14] ubuntu:636:1071 [1] graph/tuning.cc:25 NCCL WARN Invalid NCCL_NTHREADS 32 (minimum 64).\n",
      "ubuntu:636:1071 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "ubuntu:636:1071 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\n",
      "ubuntu:636:1071 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.\n",
      "ubuntu:328:1070 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.\n",
      "ubuntu:636:1071 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.\n",
      "ubuntu:636:1071 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.\n",
      "ubuntu:328:1070 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.\n",
      "ubuntu:636:1071 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.\n",
      "ubuntu:328:1070 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.\n",
      "ubuntu:636:1071 [1] NCCL INFO ncclCommInitRankConfig comm 0x3da8e1d0 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 2a000 commId 0x56b91acfc7a7dbab - Init COMPLETE\n",
      "ubuntu:328:1070 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.\n",
      "ubuntu:636:1071 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.84 (kernels 0.65, alloc 0.09, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.06, rest 0.02)\n",
      "ubuntu:328:1070 [0] NCCL INFO ncclCommInitRankConfig comm 0x37a3d210 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 27000 commId 0x56b91acfc7a7dbab - Init COMPLETE\n",
      "ubuntu:328:1070 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 2 total 0.88 (kernels 0.14, alloc 0.09, bootstrap 0.54, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.07)\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1\n",
      "ubuntu:636:1078 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1\n",
      "[NeMo I 2025-06-05 09:22:14 utils:554] Building Evo2Dataset splits with sizes=[400000, 8040, 2] and config=GPTDatasetConfig(random_seed=1234, sequence_length=1, blend=None, blend_per_split=[(['/workspace/preprocessed_data/XTT22_train_byte-level_train'], [1.0]), (['/workspace/preprocessed_data/XTT22_train_byte-level_val'], [1.0]), (['/workspace/preprocessed_data/XTT22_train_byte-level_test'], [1.0])], split=None, split_matrix=None, num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.bytelevel_tokenizers.ByteLevelTokenizer object at 0x7332dbee7740>, mid_level_dataset_surplus=0.005, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, object_storage_cache_path=None)\n",
      "[NeMo I 2025-06-05 09:22:14 utils:554] Load the _IndexReader from /workspace/preprocessed_data/XTT22_train_byte-level_train.idx\n",
      "[NeMo I 2025-06-05 09:22:14 utils:554] \tExtract the sequence lengths\n",
      "[NeMo I 2025-06-05 09:22:14 utils:554] \tExtract the sequence pointers\n",
      "[NeMo I 2025-06-05 09:22:14 utils:554] \tExtract the document indices\n",
      "[NeMo I 2025-06-05 09:22:14 utils:554] > total number of sequences: 10896\n",
      "[NeMo I 2025-06-05 09:22:14 utils:554] > total number of documents: 10896\n",
      "[NeMo I 2025-06-05 09:22:14 utils:554] Build and save the Evo2Dataset train indices\n",
      "[rank1]:[E605 09:32:14.239857255 ProcessGroupNCCL.cpp:633] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n",
      "[rank1]:[E605 09:32:14.240213264 ProcessGroupNCCL.cpp:757] [Rank 1] Work WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) timed out in blocking wait.\n",
      "ubuntu:636:1081 [1] NCCL INFO misc/socket.cc:64 -> 3\n",
      "ubuntu:636:1081 [1] NCCL INFO misc/socket.cc:80 -> 3\n",
      "ubuntu:636:1081 [1] NCCL INFO misc/socket.cc:829 -> 3\n",
      "ubuntu:636:1081 [1] NCCL INFO misc/socket.cc:64 -> 3\n",
      "ubuntu:636:1081 [1] NCCL INFO misc/socket.cc:80 -> 3\n",
      "ubuntu:636:1081 [1] NCCL INFO misc/socket.cc:829 -> 3\n",
      "ubuntu:636:1076 [1] NCCL INFO misc/socket.cc:881 -> 3\n",
      "ubuntu:636:1081 [1] NCCL INFO comm 0x3da8e1d0 rank 1 nranks 2 cudaDev 1 busId 2a000 - Abort COMPLETE\n",
      "[rank1]:[E605 09:32:15.765913519 ProcessGroupNCCL.cpp:685] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n",
      "[rank1]:[E605 09:32:15.765941872 ProcessGroupNCCL.cpp:699] [Rank 1] To avoid data inconsistency, we are taking the entire process down.\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/usr/local/bin/train_evo2\", line 10, in <module>\n",
      "[rank1]:     sys.exit(main())\n",
      "[rank1]:              ^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py\", line 1133, in main\n",
      "[rank1]:     train(args=args)\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py\", line 1126, in train\n",
      "[rank1]:     trainer.fit(model, data_module)\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\n",
      "[rank1]:     call._call_and_handle_interrupt(\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
      "[rank1]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
      "[rank1]:     return function(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\n",
      "[rank1]:     self._run(model, ckpt_path=ckpt_path)\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\", line 943, in _run\n",
      "[rank1]:     call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment\n",
      "[rank1]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\", line 102, in _call_setup_hook\n",
      "[rank1]:     _call_lightning_datamodule_hook(trainer, \"setup\", stage=fn)\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\", line 189, in _call_lightning_datamodule_hook\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/data/pre_training.py\", line 314, in setup\n",
      "[rank1]:     self.build(\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/data/pre_training.py\", line 304, in build\n",
      "[rank1]:     ).build()\n",
      "[rank1]:       ^^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 132, in build\n",
      "[rank1]:     datasets = self._build_blended_dataset_splits()\n",
      "[rank1]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 253, in _build_blended_dataset_splits\n",
      "[rank1]:     blended_datasets[i] = self._build_megatron_dataset_splits(\n",
      "[rank1]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 443, in _build_megatron_dataset_splits\n",
      "[rank1]:     self.build_generic_dataset(\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 507, in build_generic_dataset\n",
      "[rank1]:     torch.distributed.barrier()\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
      "[rank1]:     return func(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 4662, in barrier\n",
      "[rank1]:     work.wait()\n",
      "[rank1]: torch.distributed.DistBackendError: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n",
      "[rank: 1] Child process with PID 636 terminated with code 1. Forcefully terminating all other processes to avoid zombies ğŸ§Ÿ\n"
     ]
    }
   ],
   "source": [
    "# ==================== NCCLè¶…æ—¶é—®é¢˜è§£å†³æ–¹æ¡ˆ ====================\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. å¤‡ä»½å’Œæ›¿æ¢è®­ç»ƒè„šæœ¬\n",
    "print(\"ğŸ”§ å¤‡ä»½å¹¶æ›¿æ¢è®­ç»ƒè„šæœ¬...\")\n",
    "!cp /usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py /usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py.backup\n",
    "!cp /workspace/bionemo_train.py /usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py\n",
    "\n",
    "# 2. è®¾ç½®NCCLå’Œåˆ†å¸ƒå¼ç¯å¢ƒå˜é‡\n",
    "print(\"ğŸ”§ é…ç½®NCCLè¶…æ—¶å’Œä¼˜åŒ–å‚æ•°...\")\n",
    "\n",
    "# NCCLè¶…æ—¶è®¾ç½® - å¢åŠ åˆ°2å°æ—¶\n",
    "os.environ['NCCL_TIMEOUT'] = '7200'  # 2å°æ—¶è¶…æ—¶\n",
    "os.environ['TORCH_NCCL_BLOCKING_WAIT'] = '1'  # ä½¿ç”¨æ–°çš„ç¯å¢ƒå˜é‡å\n",
    "os.environ['TORCH_NCCL_ASYNC_ERROR_HANDLING'] = '1'  # ä½¿ç”¨æ–°çš„ç¯å¢ƒå˜é‡å\n",
    "os.environ['NCCL_DEBUG'] = 'INFO'  # å¯ç”¨è¯¦ç»†è°ƒè¯•ä¿¡æ¯\n",
    "\n",
    "# PyTorchåˆ†å¸ƒå¼è¶…æ—¶è®¾ç½®\n",
    "os.environ['TORCH_DISTRIBUTED_TIMEOUT'] = '7200'  # PyTorchåˆ†å¸ƒå¼è¶…æ—¶\n",
    "os.environ['TORCH_NCCL_TRACE_BUFFER_SIZE'] = '1024'  # å¯ç”¨NCCLè·Ÿè¸ª\n",
    "\n",
    "# æ•°æ®åŠ è½½å’Œé€šä¿¡ä¼˜åŒ–\n",
    "os.environ['NCCL_BUFFSIZE'] = '8388608'  # å¢åŠ ç¼“å†²åŒºå¤§å°åˆ°8MB\n",
    "os.environ['NCCL_NTHREADS'] = '8'  # å¢åŠ NCCLçº¿ç¨‹æ•°\n",
    "os.environ['NCCL_MIN_NTHREADS'] = '4'  # æœ€å°çº¿ç¨‹æ•°\n",
    "\n",
    "# é¿å…å†…å­˜ç¢ç‰‡å’Œå¹¶è¡Œå†²çª\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # é¿å…tokenizerå¹¶è¡Œå†²çª\n",
    "os.environ['OMP_NUM_THREADS'] = '4'  # é™åˆ¶OpenMPçº¿ç¨‹æ•°\n",
    "\n",
    "# æ•°æ®é›†å‡†å¤‡ä¼˜åŒ–\n",
    "os.environ['NCCL_P2P_DISABLE'] = '0'  # ç¡®ä¿P2Pé€šä¿¡å¯ç”¨\n",
    "os.environ['NCCL_SHM_DISABLE'] = '0'  # ç¡®ä¿å…±äº«å†…å­˜é€šä¿¡å¯ç”¨\n",
    "\n",
    "print(\"ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ:\")\n",
    "for key in ['NCCL_TIMEOUT', 'TORCH_DISTRIBUTED_TIMEOUT', 'NCCL_DEBUG', 'NCCL_BUFFSIZE']:\n",
    "    print(f\"  {key}: {os.environ.get(key)}\")\n",
    "\n",
    "# 3. å®šä¹‰å¸¦ç›‘æ§çš„è®­ç»ƒå‡½æ•°\n",
    "def run_training_with_monitoring():\n",
    "    \"\"\"å¸¦å®æ—¶ç›‘æ§çš„è®­ç»ƒå¯åŠ¨å‡½æ•°\"\"\"\n",
    "    \n",
    "    # è®­ç»ƒé…ç½®å‚æ•°\n",
    "    training_config = {\n",
    "        'data_config': 'training_data_config.yaml',\n",
    "        'dataset_dir': 'preprocessed_data',\n",
    "        'model_size': '7b',\n",
    "        'devices': 2,\n",
    "        'num_nodes': 1,\n",
    "        'seq_length': 1,\n",
    "        'micro_batch_size': 1,\n",
    "        'lr': 0.0001,\n",
    "        'warmup_steps': 5,\n",
    "        'max_steps': 200000,\n",
    "        'ckpt_dir': 'nemo2_evo2_7b',\n",
    "        'clip_grad': 1,\n",
    "        'wd': 0.01,\n",
    "        'activation_checkpoint_recompute_num_layers': 1,\n",
    "        'val_check_interval': 1000\n",
    "    }\n",
    "    \n",
    "    # æ„å»ºè®­ç»ƒå‘½ä»¤\n",
    "    cmd = f\"\"\"\n",
    "    train_evo2 \\\\\n",
    "        -d {training_config['data_config']} \\\\\n",
    "        --dataset-dir {training_config['dataset_dir']} \\\\\n",
    "        --model-size {training_config['model_size']} \\\\\n",
    "        --devices {training_config['devices']} \\\\\n",
    "        --num-nodes {training_config['num_nodes']} \\\\\n",
    "        --seq-length {training_config['seq_length']} \\\\\n",
    "        --micro-batch-size {training_config['micro_batch_size']} \\\\\n",
    "        --lr {training_config['lr']} \\\\\n",
    "        --warmup-steps {training_config['warmup_steps']} \\\\\n",
    "        --max-steps {training_config['max_steps']} \\\\\n",
    "        --ckpt-dir {training_config['ckpt_dir']} \\\\\n",
    "        --clip-grad {training_config['clip_grad']} \\\\\n",
    "        --wd {training_config['wd']} \\\\\n",
    "        --activation-checkpoint-recompute-num-layers {training_config['activation_checkpoint_recompute_num_layers']} \\\\\n",
    "        --val-check-interval {training_config['val_check_interval']} \\\\\n",
    "        --ckpt-async-save\n",
    "    \"\"\".strip().replace('\\n', ' ').replace('  ', ' ')\n",
    "    \n",
    "    print(f\"ğŸš€ å¼€å§‹è®­ç»ƒæ—¶é—´: {datetime.now()}\")\n",
    "    print(f\"ğŸ“‹ è®­ç»ƒå‘½ä»¤: {cmd}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # å¯åŠ¨è®­ç»ƒè¿›ç¨‹\n",
    "        process = subprocess.Popen(\n",
    "            cmd,\n",
    "            shell=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            universal_newlines=True,\n",
    "            bufsize=1\n",
    "        )\n",
    "        \n",
    "        # å®æ—¶ç›‘æ§è¾“å‡º\n",
    "        start_time = time.time()\n",
    "        last_output_time = start_time\n",
    "        dataset_preparation_detected = False\n",
    "        \n",
    "        for line in iter(process.stdout.readline, ''):\n",
    "            current_time = time.time()\n",
    "            elapsed = current_time - start_time\n",
    "            \n",
    "            # æ‰“å°å¸¦æ—¶é—´æˆ³çš„è¾“å‡º\n",
    "            print(f\"[{elapsed:.1f}s] {line.rstrip()}\")\n",
    "            \n",
    "            # æ£€æŸ¥å…³é”®ä¿¡æ¯\n",
    "            keywords = ['dataset', 'preparing', 'loading', 'barrier', 'build', 'index']\n",
    "            if any(keyword in line.lower() for keyword in keywords):\n",
    "                print(f\"ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: {line.rstrip()}\")\n",
    "                dataset_preparation_detected = True\n",
    "                last_output_time = current_time\n",
    "            \n",
    "            # NCCLç›¸å…³ä¿¡æ¯ç‰¹åˆ«æ ‡è®°\n",
    "            if 'nccl' in line.lower():\n",
    "                print(f\"ğŸ”— NCCLé€šä¿¡: {line.rstrip()}\")\n",
    "                last_output_time = current_time\n",
    "            \n",
    "            # é”™è¯¯ä¿¡æ¯ç‰¹åˆ«æ ‡è®°\n",
    "            if any(err in line.lower() for err in ['error', 'timeout', 'fail']):\n",
    "                print(f\"âŒ é”™è¯¯ä¿¡æ¯: {line.rstrip()}\")\n",
    "                last_output_time = current_time\n",
    "            \n",
    "            # é•¿æ—¶é—´æ— è¾“å‡ºçš„è­¦å‘Š\n",
    "            if current_time - last_output_time > 300:  # 5åˆ†é’Ÿæ— è¾“å‡º\n",
    "                elapsed_no_output = current_time - last_output_time\n",
    "                if dataset_preparation_detected:\n",
    "                    print(f\"\\nâ³ [æ•°æ®é›†å‡†å¤‡] å·²æœ‰ {elapsed_no_output:.1f} ç§’æ— è¾“å‡ºï¼Œæ•°æ®é›†æ„å»ºä¸­...\")\n",
    "                else:\n",
    "                    print(f\"\\nâ³ [ç­‰å¾…ä¸­] å·²æœ‰ {elapsed_no_output:.1f} ç§’æ— è¾“å‡º...\")\n",
    "                last_output_time = current_time\n",
    "        \n",
    "        # ç­‰å¾…è¿›ç¨‹å®Œæˆ\n",
    "        return_code = process.wait()\n",
    "        \n",
    "        if return_code == 0:\n",
    "            print(f\"\\nâœ… è®­ç»ƒæˆåŠŸå®Œæˆ! æ€»è€—æ—¶: {time.time() - start_time:.1f} ç§’\")\n",
    "        else:\n",
    "            print(f\"\\nâŒ è®­ç»ƒå¤±è´¥ï¼Œè¿”å›ç : {return_code}\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nğŸ›‘ ç”¨æˆ·ä¸­æ–­è®­ç»ƒ\")\n",
    "        process.terminate()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nğŸ’¥ è®­ç»ƒå‡ºé”™: {e}\")\n",
    "\n",
    "# 4. å¯åŠ¨è®­ç»ƒ\n",
    "print(\"ğŸ¯ å¯åŠ¨å¸¦ç›‘æ§çš„è®­ç»ƒ...\")\n",
    "run_training_with_monitoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf40e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== å¤‡é€‰æ–¹æ¡ˆï¼šåˆ†æ­¥éª¤è®­ç»ƒ ====================\n",
    "# å¦‚æœä¸Šé¢çš„æ–¹æ³•ä»ç„¶å‡ºç°NCCLè¶…æ—¶ï¼Œå¯ä»¥å°è¯•è¿™ä¸ªåˆ†æ­¥éª¤çš„æ–¹æ³•\n",
    "\n",
    "def run_training_with_dataset_warmup():\n",
    "    \"\"\"åˆ†æ­¥éª¤è®­ç»ƒï¼šå…ˆå•GPUé¢„çƒ­æ•°æ®é›†ï¼Œå†å¤šGPUè®­ç»ƒ\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”¥ æ­¥éª¤1: æ•°æ®é›†é¢„çƒ­ï¼ˆå•GPUæ¨¡å¼ï¼‰\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # å•GPUé¢„çƒ­å‘½ä»¤\n",
    "    warmup_cmd = \"\"\"\n",
    "    train_evo2 \\\\\n",
    "        -d training_data_config.yaml \\\\\n",
    "        --dataset-dir preprocessed_data \\\\\n",
    "        --model-size 7b \\\\\n",
    "        --devices 1 \\\\\n",
    "        --num-nodes 1 \\\\\n",
    "        --seq-length 1 \\\\\n",
    "        --micro-batch-size 1 \\\\\n",
    "        --max-steps 1 \\\\\n",
    "        --ckpt-dir warmup_ckpt \\\\\n",
    "        --val-check-interval 1\n",
    "    \"\"\".strip().replace('\\n', ' ').replace('  ', ' ')\n",
    "    \n",
    "    print(f\"é¢„çƒ­å‘½ä»¤: {warmup_cmd}\")\n",
    "    \n",
    "    # æ‰§è¡Œé¢„çƒ­\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            warmup_cmd, \n",
    "            shell=True, \n",
    "            capture_output=True, \n",
    "            text=True, \n",
    "            timeout=1800  # 30åˆ†é’Ÿè¶…æ—¶\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… æ•°æ®é›†é¢„çƒ­æˆåŠŸ!\")\n",
    "            print(\"ğŸš€ æ­¥éª¤2: å¼€å§‹æ­£å¼å¤šGPUè®­ç»ƒ\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # æ¸…ç†é¢„çƒ­çš„checkpoint\n",
    "            !rm -rf warmup_ckpt\n",
    "            \n",
    "            # ç°åœ¨è¿è¡Œæ­£å¸¸çš„å¤šGPUè®­ç»ƒ\n",
    "            run_training_with_monitoring()\n",
    "        else:\n",
    "            print(f\"âŒ æ•°æ®é›†é¢„çƒ­å¤±è´¥:\")\n",
    "            print(f\"è¿”å›ç : {result.returncode}\")\n",
    "            print(f\"é”™è¯¯è¾“å‡º: {result.stderr}\")\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"âŒ æ•°æ®é›†é¢„çƒ­è¶…æ—¶ï¼ˆ30åˆ†é’Ÿï¼‰\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ•°æ®é›†é¢„çƒ­å‡ºé”™: {e}\")\n",
    "\n",
    "# å–æ¶ˆæ³¨é‡Šä¸‹é¢çš„è¡Œæ¥ä½¿ç”¨åˆ†æ­¥éª¤æ–¹æ³•\n",
    "# print(\"ğŸ”„ å¯åŠ¨åˆ†æ­¥éª¤è®­ç»ƒ...\")\n",
    "# run_training_with_dataset_warmup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== æ•…éšœæ’é™¤å·¥å…· ====================\n",
    "# ç”¨äºè¯Šæ–­NCCLå’Œè®­ç»ƒé—®é¢˜\n",
    "\n",
    "def diagnose_system():\n",
    "    \"\"\"ç³»ç»Ÿè¯Šæ–­å’Œæ•…éšœæ’é™¤\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” ç³»ç»Ÿè¯Šæ–­æŠ¥å‘Š\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # æ£€æŸ¥GPUçŠ¶æ€\n",
    "    print(\"ğŸ“Ÿ GPUçŠ¶æ€:\")\n",
    "    !nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu --format=csv,noheader,nounits\n",
    "    \n",
    "    print(\"\\nğŸ’¾ å†…å­˜ä½¿ç”¨:\")\n",
    "    !free -h | head -2\n",
    "    \n",
    "    print(\"\\nğŸ’½ ç£ç›˜ç©ºé—´:\")\n",
    "    !df -h | grep -E '^/dev|File'\n",
    "    \n",
    "    print(\"\\nğŸ”— ç½‘ç»œæ¥å£:\")\n",
    "    !ip addr show | grep -E '^[0-9]+:|inet '\n",
    "    \n",
    "    print(\"\\nğŸ”§ NCCLç¯å¢ƒå˜é‡:\")\n",
    "    !env | grep -E '^(NCCL|TORCH)' | sort\n",
    "    \n",
    "    print(\"\\nğŸ“Š è¿›ç¨‹çŠ¶æ€:\")\n",
    "    !ps aux | grep -E '(train_evo2|python.*train)' | grep -v grep\n",
    "    \n",
    "    print(\"\\nğŸ PyTorchå’ŒCUDAç‰ˆæœ¬:\")\n",
    "    import torch\n",
    "    print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "    print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "    print(f\"CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "    print(f\"CUDAè®¾å¤‡æ•°: {torch.cuda.device_count()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"    å†…å­˜: {torch.cuda.get_device_properties(i).total_memory // 1024**3} GB\")\n",
    "\n",
    "def check_nccl_connectivity():\n",
    "    \"\"\"æµ‹è¯•NCCLè¿æ¥æ€§\"\"\"\n",
    "    print(\"\\nğŸ”— NCCLè¿æ¥æ€§æµ‹è¯•:\")\n",
    "    \n",
    "    test_script = '''\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() >= 2:\n",
    "    try:\n",
    "        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "        os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "        \n",
    "        # æµ‹è¯•NCCLåˆå§‹åŒ–\n",
    "        dist.init_process_group(\"nccl\", rank=0, world_size=1)\n",
    "        print(\"âœ… NCCLåˆå§‹åŒ–æˆåŠŸ\")\n",
    "        \n",
    "        # æµ‹è¯•tensoræ“ä½œ\n",
    "        tensor = torch.ones(2).cuda()\n",
    "        print(f\"âœ… CUDA tensoråˆ›å»ºæˆåŠŸ: {tensor}\")\n",
    "        \n",
    "        dist.destroy_process_group()\n",
    "        print(\"âœ… NCCLæ¸…ç†æˆåŠŸ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ NCCLæµ‹è¯•å¤±è´¥: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ éœ€è¦è‡³å°‘2ä¸ªGPUæ¥è¿›è¡ŒNCCLæµ‹è¯•\")\n",
    "'''\n",
    "    \n",
    "    exec(test_script)\n",
    "\n",
    "# è¿è¡Œè¯Šæ–­\n",
    "diagnose_system()\n",
    "check_nccl_connectivity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e795111f-d548-4d61-a1c1-85d6e95a2ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
