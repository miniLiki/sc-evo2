{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "concat_path = \"XTT22_train.fa\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_fasta_path = os.path.abspath(concat_path)\n",
        "output_dir = os.path.abspath(\"preprocessed_data\")\n",
        "output_yaml = f\"\"\"\n",
        "- datapaths: [\"{full_fasta_path}\"]\n",
        "  output_dir: \"{output_dir}\"\n",
        "  output_prefix: XTT22_train\n",
        "  train_split: 0.9\n",
        "  valid_split: 0.05\n",
        "  test_split: 0.05\n",
        "  overwrite: True\n",
        "  embed_reverse_complement: true\n",
        "  random_reverse_complement: 0.0\n",
        "  random_lineage_dropout: 0.0\n",
        "  include_sequence_id: false\n",
        "  transcribe: \"back_transcribe\"\n",
        "  force_uppercase: false\n",
        "  indexed_dataset_dtype: \"uint8\"\n",
        "  tokenizer_type: \"Byte-Level\"\n",
        "  vocab_file: null\n",
        "  vocab_size: null\n",
        "  merges_file: null\n",
        "  pretrained_tokenizer_model: null\n",
        "  special_tokens: null\n",
        "  fast_hf_tokenizer: true\n",
        "  append_eod: true\n",
        "  enforce_sample_length: null\n",
        "  ftfy: false\n",
        "  workers: 1\n",
        "  preproc_concurrency: 100000\n",
        "  chunksize: 25\n",
        "  drop_empty_sequences: true\n",
        "  nnn_filter: false  # If you split your fasta on NNN (in human these are contigs), then you should set this to true.\n",
        "  seed: 12342  # Not relevant because we are not using random reverse complement or lineage dropout.\n",
        "\"\"\"\n",
        "with open(\"preprocess_config.yaml\", \"w\") as f:\n",
        "    print(output_yaml, file=f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!preprocess_evo2 --config preprocess_config.yaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ls -lh preprocessed_data/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!cp /workspace/hyena_modified.py /usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/model/hyena.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!evo2_convert_to_nemo2 \\\n",
        "  --model-path /workspace/savanna_evo2_7b/savanna_evo2_7b.pt \\\n",
        "  --model-size 7b --output-dir nemo2_evo2_7b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== NCCLè¶…æ—¶é—®é¢˜è§£å†³æ–¹æ¡ˆ ====================\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# 1. å¤‡ä»½å’Œæ›¿æ¢è®­ç»ƒè„šæœ¬\n",
        "print(\"ğŸ”§ å¤‡ä»½å¹¶æ›¿æ¢è®­ç»ƒè„šæœ¬...\")\n",
        "!cp /usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py /usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py.backup 2>/dev/null || echo \"å¤‡ä»½æ–‡ä»¶å·²å­˜åœ¨æˆ–è·¯å¾„ä¸å­˜åœ¨\"\n",
        "!cp /workspace/bionemo_train.py /usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py 2>/dev/null || echo \"è‡ªå®šä¹‰è®­ç»ƒè„šæœ¬ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤ç‰ˆæœ¬\"\n",
        "\n",
        "# 2. è®¾ç½®NCCLå’Œåˆ†å¸ƒå¼ç¯å¢ƒå˜é‡\n",
        "print(\"ğŸ”§ é…ç½®NCCLè¶…æ—¶å’Œä¼˜åŒ–å‚æ•°...\")\n",
        "\n",
        "# NCCLè¶…æ—¶è®¾ç½® - å¢åŠ åˆ°2å°æ—¶\n",
        "os.environ['NCCL_TIMEOUT'] = '7200'  # 2å°æ—¶è¶…æ—¶\n",
        "os.environ['TORCH_NCCL_BLOCKING_WAIT'] = '1'  # ä½¿ç”¨æ–°çš„ç¯å¢ƒå˜é‡å\n",
        "os.environ['TORCH_NCCL_ASYNC_ERROR_HANDLING'] = '1'  # ä½¿ç”¨æ–°çš„ç¯å¢ƒå˜é‡å\n",
        "os.environ['NCCL_DEBUG'] = 'INFO'  # å¯ç”¨è¯¦ç»†è°ƒè¯•ä¿¡æ¯\n",
        "\n",
        "# PyTorchåˆ†å¸ƒå¼è¶…æ—¶è®¾ç½®\n",
        "os.environ['TORCH_DISTRIBUTED_TIMEOUT'] = '7200'  # PyTorchåˆ†å¸ƒå¼è¶…æ—¶\n",
        "os.environ['TORCH_NCCL_TRACE_BUFFER_SIZE'] = '1024'  # å¯ç”¨NCCLè·Ÿè¸ª\n",
        "\n",
        "# æ•°æ®åŠ è½½å’Œé€šä¿¡ä¼˜åŒ–\n",
        "os.environ['NCCL_BUFFSIZE'] = '8388608'  # å¢åŠ ç¼“å†²åŒºå¤§å°åˆ°8MB\n",
        "os.environ['NCCL_NTHREADS'] = '8'  # å¢åŠ NCCLçº¿ç¨‹æ•°\n",
        "os.environ['NCCL_MIN_NTHREADS'] = '4'  # æœ€å°çº¿ç¨‹æ•°\n",
        "\n",
        "# é¿å…å†…å­˜ç¢ç‰‡å’Œå¹¶è¡Œå†²çª\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # é¿å…tokenizerå¹¶è¡Œå†²çª\n",
        "os.environ['OMP_NUM_THREADS'] = '4'  # é™åˆ¶OpenMPçº¿ç¨‹æ•°\n",
        "\n",
        "# æ•°æ®é›†å‡†å¤‡ä¼˜åŒ–\n",
        "os.environ['NCCL_P2P_DISABLE'] = '0'  # ç¡®ä¿P2Pé€šä¿¡å¯ç”¨\n",
        "os.environ['NCCL_SHM_DISABLE'] = '0'  # ç¡®ä¿å…±äº«å†…å­˜é€šä¿¡å¯ç”¨\n",
        "\n",
        "print(\"ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ:\")\n",
        "for key in ['NCCL_TIMEOUT', 'TORCH_DISTRIBUTED_TIMEOUT', 'NCCL_DEBUG', 'NCCL_BUFFSIZE']:\n",
        "    print(f\"  {key}: {os.environ.get(key)}\")\n",
        "\n",
        "# 3. å®šä¹‰å¸¦ç›‘æ§çš„è®­ç»ƒå‡½æ•°\n",
        "def run_training_with_monitoring():\n",
        "    \"\"\"å¸¦å®æ—¶ç›‘æ§çš„è®­ç»ƒå¯åŠ¨å‡½æ•°\"\"\"\n",
        "    \n",
        "    # è·å–å½“å‰å·¥ä½œç›®å½•ä¸­çš„preprocessed_dataè·¯å¾„\n",
        "    preprocessed_data = os.path.abspath(\"preprocessed_data\")\n",
        "    print(f\"ğŸ“ æ•°æ®é›†ç›®å½•: {preprocessed_data}\")\n",
        "    \n",
        "    # è®­ç»ƒé…ç½®å‚æ•°\n",
        "    training_config = {\n",
        "        'data_config': 'training_data_config.yaml',\n",
        "        'dataset_dir': preprocessed_data,  # ä½¿ç”¨å®é™…è·¯å¾„\n",
        "        'model_size': '7b',\n",
        "        'devices': 2,\n",
        "        'num_nodes': 1,\n",
        "        'seq_length': 1,\n",
        "        'micro_batch_size': 1,\n",
        "        'lr': 0.0001,\n",
        "        'warmup_steps': 5,\n",
        "        'max_steps': 200000,\n",
        "        'ckpt_dir': 'nemo2_evo2_7b',\n",
        "        'clip_grad': 1,\n",
        "        'wd': 0.01,\n",
        "        'activation_checkpoint_recompute_num_layers': 1,\n",
        "        'val_check_interval': 1000\n",
        "    }\n",
        "    \n",
        "    # æ„å»ºè®­ç»ƒå‘½ä»¤ - ä½¿ç”¨æ­£ç¡®çš„æ ¼å¼\n",
        "    cmd_parts = [\n",
        "        'train_evo2',\n",
        "        '-d', training_config['data_config'],\n",
        "        '--dataset-dir', training_config['dataset_dir'],\n",
        "        '--model-size', training_config['model_size'],\n",
        "        '--devices', str(training_config['devices']),\n",
        "        '--num-nodes', str(training_config['num_nodes']),\n",
        "        '--seq-length', str(training_config['seq_length']),\n",
        "        '--micro-batch-size', str(training_config['micro_batch_size']),\n",
        "        '--lr', str(training_config['lr']),\n",
        "        '--warmup-steps', str(training_config['warmup_steps']),\n",
        "        '--max-steps', str(training_config['max_steps']),\n",
        "        '--ckpt-dir', training_config['ckpt_dir'],\n",
        "        '--clip-grad', str(training_config['clip_grad']),\n",
        "        '--wd', str(training_config['wd']),\n",
        "        '--activation-checkpoint-recompute-num-layers', str(training_config['activation_checkpoint_recompute_num_layers']),\n",
        "        '--val-check-interval', str(training_config['val_check_interval']),\n",
        "        '--ckpt-async-save'\n",
        "    ]\n",
        "    \n",
        "    cmd = ' '.join(cmd_parts)\n",
        "    \n",
        "    print(f\"ğŸš€ å¼€å§‹è®­ç»ƒæ—¶é—´: {datetime.now()}\")\n",
        "    print(f\"ğŸ“‹ è®­ç»ƒå‘½ä»¤: {cmd}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    try:\n",
        "        # å¯åŠ¨è®­ç»ƒè¿›ç¨‹\n",
        "        process = subprocess.Popen(\n",
        "            cmd_parts,  # ä½¿ç”¨åˆ—è¡¨è€Œä¸æ˜¯å­—ç¬¦ä¸²ï¼Œé¿å…shellè§£æé—®é¢˜\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            universal_newlines=True,\n",
        "            bufsize=1\n",
        "        )\n",
        "        \n",
        "        # å®æ—¶ç›‘æ§è¾“å‡º\n",
        "        start_time = time.time()\n",
        "        last_output_time = start_time\n",
        "        dataset_preparation_detected = False\n",
        "        \n",
        "        for line in iter(process.stdout.readline, ''):\n",
        "            current_time = time.time()\n",
        "            elapsed = current_time - start_time\n",
        "            \n",
        "            # æ‰“å°å¸¦æ—¶é—´æˆ³çš„è¾“å‡º\n",
        "            print(f\"[{elapsed:.1f}s] {line.rstrip()}\")\n",
        "            \n",
        "            # æ£€æŸ¥å…³é”®ä¿¡æ¯\n",
        "            keywords = ['dataset', 'preparing', 'loading', 'barrier', 'build', 'index']\n",
        "            if any(keyword in line.lower() for keyword in keywords):\n",
        "                print(f\"ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: {line.rstrip()}\")\n",
        "                dataset_preparation_detected = True\n",
        "                last_output_time = current_time\n",
        "            \n",
        "            # NCCLç›¸å…³ä¿¡æ¯ç‰¹åˆ«æ ‡è®°\n",
        "            if 'nccl' in line.lower():\n",
        "                print(f\"ğŸ”— NCCLé€šä¿¡: {line.rstrip()}\")\n",
        "                last_output_time = current_time\n",
        "            \n",
        "            # é”™è¯¯ä¿¡æ¯ç‰¹åˆ«æ ‡è®°\n",
        "            if any(err in line.lower() for err in ['error', 'timeout', 'fail']):\n",
        "                print(f\"âŒ é”™è¯¯ä¿¡æ¯: {line.rstrip()}\")\n",
        "                last_output_time = current_time\n",
        "            \n",
        "            # é•¿æ—¶é—´æ— è¾“å‡ºçš„è­¦å‘Š\n",
        "            if current_time - last_output_time > 300:  # 5åˆ†é’Ÿæ— è¾“å‡º\n",
        "                elapsed_no_output = current_time - last_output_time\n",
        "                if dataset_preparation_detected:\n",
        "                    print(f\"\\nâ³ [æ•°æ®é›†å‡†å¤‡] å·²æœ‰ {elapsed_no_output:.1f} ç§’æ— è¾“å‡ºï¼Œæ•°æ®é›†æ„å»ºä¸­...\")\n",
        "                else:\n",
        "                    print(f\"\\nâ³ [ç­‰å¾…ä¸­] å·²æœ‰ {elapsed_no_output:.1f} ç§’æ— è¾“å‡º...\")\n",
        "                last_output_time = current_time\n",
        "        \n",
        "        # ç­‰å¾…è¿›ç¨‹å®Œæˆ\n",
        "        return_code = process.wait()\n",
        "        \n",
        "        if return_code == 0:\n",
        "            print(f\"\\nâœ… è®­ç»ƒæˆåŠŸå®Œæˆ! æ€»è€—æ—¶: {time.time() - start_time:.1f} ç§’\")\n",
        "        else:\n",
        "            print(f\"\\nâŒ è®­ç»ƒå¤±è´¥ï¼Œè¿”å›ç : {return_code}\")\n",
        "            \n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nğŸ›‘ ç”¨æˆ·ä¸­æ–­è®­ç»ƒ\")\n",
        "        process.terminate()\n",
        "    except Exception as e:\n",
        "        print(f\"\\nğŸ’¥ è®­ç»ƒå‡ºé”™: {e}\")\n",
        "\n",
        "# 4. å¯åŠ¨è®­ç»ƒ\n",
        "print(\"ğŸ¯ å¯åŠ¨å¸¦ç›‘æ§çš„è®­ç»ƒ...\")\n",
        "run_training_with_monitoring()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
