{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f816cb-6abb-4cf9-9a0b-d1382a4c9910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "concat_path = \"XTT22_train.fa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a015b5-fad5-4d11-adcd-2fb18c594536",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_fasta_path = os.path.abspath(concat_path)\n",
    "output_dir = os.path.abspath(\"preprocessed_data\")\n",
    "output_yaml = f\"\"\"\n",
    "- datapaths: [\"{full_fasta_path}\"]\n",
    "  output_dir: \"{output_dir}\"\n",
    "  output_prefix: XTT22_train\n",
    "  train_split: 0.9\n",
    "  valid_split: 0.05\n",
    "  test_split: 0.05\n",
    "  overwrite: True\n",
    "  embed_reverse_complement: true\n",
    "  random_reverse_complement: 0.0\n",
    "  random_lineage_dropout: 0.0\n",
    "  include_sequence_id: false\n",
    "  transcribe: \"back_transcribe\"\n",
    "  force_uppercase: false\n",
    "  indexed_dataset_dtype: \"uint8\"\n",
    "  tokenizer_type: \"Byte-Level\"\n",
    "  vocab_file: null\n",
    "  vocab_size: null\n",
    "  merges_file: null\n",
    "  pretrained_tokenizer_model: null\n",
    "  special_tokens: null\n",
    "  fast_hf_tokenizer: true\n",
    "  append_eod: true\n",
    "  enforce_sample_length: null\n",
    "  ftfy: false\n",
    "  workers: 1\n",
    "  preproc_concurrency: 100000\n",
    "  chunksize: 25\n",
    "  drop_empty_sequences: true\n",
    "  nnn_filter: false  # If you split your fasta on NNN (in human these are contigs), then you should set this to true.\n",
    "  seed: 12342  # Not relevant because we are not using random reverse complement or lineage dropout.\n",
    "\"\"\"\n",
    "with open(\"preprocess_config.yaml\", \"w\") as f:\n",
    "    print(output_yaml, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aadbf5cb-03b5-4a20-96e5-7bb0a7e5b942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-24 12:37:06 nemo_logging:393] Using byte-level tokenization\n",
      "[NeMo I 2025-05-24 12:37:06 nemo_logging:393] Created temporary binary datasets: /workspace/preprocessed_data/XTT22_train_byte-level_train.bin.tmp /workspace/preprocessed_data/XTT22_train_byte-level_val.bin.tmp /workspace/preprocessed_data/XTT22_train_byte-level_test.bin.tmp\n",
      "[NeMo I 2025-05-24 13:12:11 nemo_logging:393] Average preprocessing time per sequence: 0.04470627161196968\n",
      "[NeMo I 2025-05-24 13:12:11 nemo_logging:393] Average indexing time per sequence: 0.1463382052460373\n",
      "[NeMo I 2025-05-24 13:12:11 nemo_logging:393] Number of sequences processed: 12092\n",
      "[NeMo I 2025-05-24 13:12:11 nemo_logging:393] Finished preprocessing XTT22_train ([PosixPath('/workspace/XTT22_train.fa')]) in 2105.082 seconds with 1 workers.\n"
     ]
    }
   ],
   "source": [
    "!preprocess_evo2 --config preprocess_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3df5d34a-d5fe-4fa9-ac3d-65b9696a9045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14G\n",
      "-rw-r--r-- 1 root root 936M May 24 13:11 XTT22_train_byte-level_test.bin\n",
      "-rw-r--r-- 1 root root  12K May 24 13:12 XTT22_train_byte-level_test.idx\n",
      "-rw-r--r-- 1 root root  13G May 24 13:12 XTT22_train_byte-level_train.bin\n",
      "-rw-r--r-- 1 root root 213K May 24 13:12 XTT22_train_byte-level_train.idx\n",
      "-rw-r--r-- 1 root root 411M May 24 13:12 XTT22_train_byte-level_val.bin\n",
      "-rw-r--r-- 1 root root  12K May 24 13:12 XTT22_train_byte-level_val.idx\n"
     ]
    }
   ],
   "source": [
    "!ls -lh preprocessed_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec85fb63-1214-45f4-849b-87f54166d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /workspace/hyena_modified.py /usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/model/hyena.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a20f4f4-8c19-44b7-94b5-db53eff61976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda129.so')\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Using byte-level tokenization\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2025-06-02 07:46:00 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "    \n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo W 2025-06-02 07:46:00 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py:1090: `trainer.init_module` cannot fully support proper instantiation of your model with the `MegatronStrategy` strategy. Please instantiate your model inside the`LightningModule.configure_model` hook instead\n",
      "    \n",
      "[NeMo I 2025-06-02 07:46:00 nemo_logging:393] Padded vocab_size: 512, original vocab_size: 512, dummy tokens: 0.\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.2.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.6.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.9.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.13.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.16.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.20.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.23.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.27.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:04 nemo_logging:405] Converting sequential.30.mixer.mixer.filter.t from torch.float32 (source model) to torch.bfloat16 (target model)\n",
      "[NeMo W 2025-06-02 07:46:05 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n",
      "[NeMo I 2025-06-02 07:46:05 nemo_logging:393] Using FullyParallelSaveStrategyWrapper(torch_dist, 1) dist-ckpt save strategy.\n",
      "Overwriting old incomplete / corrupted checkpoint...\n",
      "[NeMo I 2025-06-02 07:46:18 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 0 : Start time: 1748850365.187s : Save duration: 12.931s\n",
      "[NeMo I 2025-06-02 07:46:18 nemo_logging:393] Converted Hyena model to Nemo, model saved to nemo2_evo2_7b\n"
     ]
    }
   ],
   "source": [
    "!evo2_convert_to_nemo2 \\\n",
    "  --model-path /workspace/savanna_evo2_7b/savanna_evo2_7b.pt \\\n",
    "  --model-size 7b --output-dir nemo2_evo2_7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adacfca1-de37-4ece-919b-c720dc285ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 设置PyTorch分布式超时: 7200秒\n",
      "Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda129.so')\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Using byte-level tokenization\n",
      "启用 LoRA 微调...\n",
      "模型结构调试信息:\n",
      "--------------------------------------------------\n",
      "模型总共有 1 个模块\n",
      "尝试访问模型的其他属性...\n",
      "模块结构:\n",
      "   1.  (HyenaModel)\n",
      "\n",
      "目标模式: ['module.decoder.layers.17.self_attention.linear_qkv']\n",
      "\n",
      "所有线性层 (0):\n",
      "\n",
      "找到的注意力相关层 (0):\n",
      "\n",
      "找到的线性层 (0):\n",
      "\n",
      "找到的QKV层 (0):\n",
      "⚠️  没有找到任何线性层！模型可能还没有完全初始化。\n",
      "这在 NeMo/Megatron 框架中是正常的，模型结构会在训练开始时初始化。\n",
      "LoRA 配置已保存，将在模型完全初始化后应用。\n",
      "总参数数量: 0\n",
      "可训练参数数量: 0\n",
      "⚠️  警告: 模型总参数数量为0，可能模型初始化有问题\n",
      "⚠️  警告: 没有可训练的参数，LoRA 可能没有正确应用\n",
      "LoRA 将在训练开始时自动应用\n",
      "已添加 LoRA 回调，将在训练开始时应用 LoRA\n",
      "[NeMo W 2025-06-05 09:21:58 nemo_logging:405] WandB is currently turned off.\n",
      "[NeMo W 2025-06-05 09:21:58 nemo_logging:405] User-set tensorboard is currently turned off. Internally one may still be set by NeMo2.\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Experiments will be logged at results/evo2/dev\n",
      "[NeMo W 2025-06-05 09:21:58 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :results/evo2/checkpoints. Training from scratch.\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has data parallel group : [0, 1]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0, 1]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1]]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] All context parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] All model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] All tensor model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] All embedding group ranks: [[0], [1]]\n",
      "[NeMo I 2025-06-05 09:21:58 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "🔧 设置PyTorch分布式超时: 7200秒\n",
      "Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda129.so')\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "启用 LoRA 微调...\n",
      "模型结构调试信息:\n",
      "--------------------------------------------------\n",
      "模型总共有 1 个模块\n",
      "尝试访问模型的其他属性...\n",
      "模块结构:\n",
      "   1.  (HyenaModel)\n",
      "\n",
      "目标模式: ['module.decoder.layers.17.self_attention.linear_qkv']\n",
      "\n",
      "所有线性层 (0):\n",
      "\n",
      "找到的注意力相关层 (0):\n",
      "\n",
      "找到的线性层 (0):\n",
      "\n",
      "找到的QKV层 (0):\n",
      "⚠️  没有找到任何线性层！模型可能还没有完全初始化。\n",
      "这在 NeMo/Megatron 框架中是正常的，模型结构会在训练开始时初始化。\n",
      "LoRA 配置已保存，将在模型完全初始化后应用。\n",
      "总参数数量: 0\n",
      "可训练参数数量: 0\n",
      "⚠️  警告: 模型总参数数量为0，可能模型初始化有问题\n",
      "⚠️  警告: 没有可训练的参数，LoRA 可能没有正确应用\n",
      "LoRA 将在训练开始时自动应用\n",
      "已添加 LoRA 回调，将在训练开始时应用 LoRA\n",
      "[W605 09:22:13.707703116 Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n",
      "[W605 09:22:13.707728252 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "[W605 09:22:13.715988247 Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n",
      "[W605 09:22:13.716068477 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "ubuntu:328:328 [0] NCCL INFO Bootstrap: Using eth0:172.21.72.20<0>\n",
      "ubuntu:328:328 [0] NCCL INFO cudaDriverVersion 12090\n",
      "ubuntu:328:328 [0] NCCL INFO NCCL version 2.26.3+cuda12.9\n",
      "ubuntu:328:328 [0] NCCL INFO Comm config Blocking set to 1\n",
      "ubuntu:636:636 [1] NCCL INFO cudaDriverVersion 12090\n",
      "ubuntu:636:636 [1] NCCL INFO Bootstrap: Using eth0:172.21.72.20<0>\n",
      "ubuntu:636:636 [1] NCCL INFO NCCL version 2.26.3+cuda12.9\n",
      "ubuntu:636:636 [1] NCCL INFO Comm config Blocking set to 1\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v10 symbol.\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v9 (v9)\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/Plugin: Loaded collnet plugin SHARP (v9)\n",
      "ubuntu:328:1070 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\n",
      "ubuntu:328:1070 [0] NCCL INFO P2P plugin v9 IBext_v9\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/IB : No device found.\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/IB : No device found.\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "ubuntu:328:1070 [0] NCCL INFO NET/Socket : Using [0]eth0:172.21.72.20<0> [1]eth1:172.25.58.75<0> [2]nodelocaldns:169.254.25.10<0> [3]vxlan.calico:10.233.112.0<0> [4]cali829ebf906e6:fe80::ecee:eeff:feee:eeee%cali829ebf906e6<0> [5]cali9ffd33e0038:fe80::ecee:eeff:feee:eeee%cali9ffd33e0038<0> [6]cali210c644397a:fe80::ecee:eeff:feee:eeee%cali210c644397a<0> [7]caliafd05346b22:fe80::ecee:eeff:feee:eeee%caliafd05346b22<0> [8]calicd149c44f0f:fe80::ecee:eeff:feee:eeee%calicd149c44f0f<0> [9]cali62597977d84:fe80::ecee:eeff:feee:eeee%cali62597977d84<0> [10]calidee47cce1bb:fe80::ecee:eeff:feee:eeee%calidee47cce1bb<0> [11]cali2ad9843228b:fe80::ecee:eeff:feee:eeee%cali2ad9843228b<0> [12]calicb98c4ca0f1:fe80::ecee:eeff:feee:eeee%calicb98c4ca0f1<0> [13]cali9c537539f6f:fe80::ecee:eeff:feee:eeee%cali9c537539f6f<0> [14]cali3dd684f6336:fe80::ecee:eeff:feee:eeee%cali3dd684f6336<0> [15]calid294f721236:fe80::ecee:eeff:feee:eeee%calid294f721236<0>\n",
      "ubuntu:328:1070 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. \n",
      "ubuntu:328:1070 [0] NCCL INFO Using network Socket\n",
      "ubuntu:328:1070 [0] NCCL INFO ncclCommInitRankConfig comm 0x37a3d210 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 27000 commId 0x56b91acfc7a7dbab - Init START\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v10 symbol.\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v9 (v9)\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/Plugin: Loaded collnet plugin SHARP (v9)\n",
      "ubuntu:636:1071 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\n",
      "ubuntu:636:1071 [1] NCCL INFO P2P plugin v9 IBext_v9\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/IB : No device found.\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/IB : No device found.\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "ubuntu:636:1071 [1] NCCL INFO NET/Socket : Using [0]eth0:172.21.72.20<0> [1]eth1:172.25.58.75<0> [2]nodelocaldns:169.254.25.10<0> [3]vxlan.calico:10.233.112.0<0> [4]cali829ebf906e6:fe80::ecee:eeff:feee:eeee%cali829ebf906e6<0> [5]cali9ffd33e0038:fe80::ecee:eeff:feee:eeee%cali9ffd33e0038<0> [6]cali210c644397a:fe80::ecee:eeff:feee:eeee%cali210c644397a<0> [7]caliafd05346b22:fe80::ecee:eeff:feee:eeee%caliafd05346b22<0> [8]calicd149c44f0f:fe80::ecee:eeff:feee:eeee%calicd149c44f0f<0> [9]cali62597977d84:fe80::ecee:eeff:feee:eeee%cali62597977d84<0> [10]calidee47cce1bb:fe80::ecee:eeff:feee:eeee%calidee47cce1bb<0> [11]cali2ad9843228b:fe80::ecee:eeff:feee:eeee%cali2ad9843228b<0> [12]calicb98c4ca0f1:fe80::ecee:eeff:feee:eeee%calicb98c4ca0f1<0> [13]cali9c537539f6f:fe80::ecee:eeff:feee:eeee%cali9c537539f6f<0> [14]cali3dd684f6336:fe80::ecee:eeff:feee:eeee%cali3dd684f6336<0> [15]calid294f721236:fe80::ecee:eeff:feee:eeee%calid294f721236<0>\n",
      "ubuntu:636:1071 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. \n",
      "ubuntu:636:1071 [1] NCCL INFO Using network Socket\n",
      "ubuntu:636:1071 [1] NCCL INFO ncclCommInitRankConfig comm 0x3da8e1d0 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 2a000 commId 0x56b91acfc7a7dbab - Init START\n",
      "ubuntu:636:1071 [1] NCCL INFO RAS client listening socket at 127.0.0.1<28028>\n",
      "ubuntu:328:1070 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>\n",
      "ubuntu:636:1071 [1] NCCL INFO Bootstrap timings total 0.001530 (create 0.000040, send 0.000209, recv 0.000707, ring 0.000204, delay 0.000001)\n",
      "ubuntu:328:1070 [0] NCCL INFO Bootstrap timings total 0.543683 (create 0.000045, send 0.016664, recv 0.526136, ring 0.000074, delay 0.000001)\n",
      "ubuntu:636:1071 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff\n",
      "ubuntu:328:1070 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff\n",
      "ubuntu:636:1071 [1] NCCL INFO comm 0x3da8e1d0 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0\n",
      "ubuntu:328:1070 [0] NCCL INFO comm 0x37a3d210 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 00/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 01/24 : 0 1\n",
      "ubuntu:636:1071 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] 0/-1/-1->1->-1 [7] 0/-1/-1->1->-1 [8] 0/-1/-1->1->-1 [9] 0/-1/-1->1->-1 [10] 0/-1/-1->1->-1 [11] 0/-1/-1->1->-1 [12] -1/-1/-1->1->0 [13] -1/-1/-1->1->0 [14] -1/-1/-1->1->0 [15] -1/-1/-1->1->0 [16] -1/-1/-1->1->0 [17] -1/-1/-1->1->0 [18] 0/-1/-1->1->-1 [19] 0/-1/-1->1->-1 [20] 0/-1/-1->1->-1 [21] 0/-1/-1->1->-1 [22] 0/-1/-1->1->-1 [23] 0/-1/-1->1->-1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 02/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 03/24 : 0 1\n",
      "ubuntu:636:1071 [1] NCCL INFO NCCL_BUFFSIZE set by environment to 8388608.\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 04/24 : 0 1\n",
      "ubuntu:636:1071 [1] NCCL INFO P2P Chunksize set to 524288\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 05/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 06/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 07/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 08/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 09/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 10/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 11/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 12/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 13/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 14/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 15/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 16/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 17/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 18/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 19/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 20/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 21/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 22/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Channel 23/24 : 0 1\n",
      "ubuntu:328:1070 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] -1/-1/-1->0->1 [9] -1/-1/-1->0->1 [10] -1/-1/-1->0->1 [11] -1/-1/-1->0->1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] -1/-1/-1->0->1 [19] -1/-1/-1->0->1 [20] -1/-1/-1->0->1 [21] -1/-1/-1->0->1 [22] -1/-1/-1->0->1 [23] -1/-1/-1->0->1\n",
      "ubuntu:328:1070 [0] NCCL INFO NCCL_BUFFSIZE set by environment to 8388608.\n",
      "ubuntu:328:1070 [0] NCCL INFO P2P Chunksize set to 524288\n",
      "ubuntu:328:1070 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 1 directMode 0\n",
      "ubuntu:328:1075 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 92\n",
      "ubuntu:328:1074 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12\n",
      "ubuntu:328:1070 [0] NCCL INFO NCCL_NTHREADS set by environment to 32.\n",
      "\n",
      "[2025-06-05 09:22:14] ubuntu:328:1070 [0] graph/tuning.cc:25 NCCL WARN Invalid NCCL_NTHREADS 32 (minimum 64).\n",
      "\n",
      "[2025-06-05 09:22:14] ubuntu:328:1070 [0] graph/tuning.cc:25 NCCL WARN Invalid NCCL_NTHREADS 32 (minimum 64).\n",
      "\n",
      "[2025-06-05 09:22:14] ubuntu:328:1070 [0] graph/tuning.cc:25 NCCL WARN Invalid NCCL_NTHREADS 32 (minimum 64).\n",
      "ubuntu:328:1070 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "ubuntu:328:1070 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\n",
      "ubuntu:328:1070 [0] NCCL INFO CC Off, workFifoBytes 1048576\n",
      "ubuntu:636:1076 [1] NCCL INFO [Proxy Service] Device 1 CPU core 8\n",
      "ubuntu:636:1077 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 74\n",
      "ubuntu:636:1071 [1] NCCL INFO NCCL_NTHREADS set by environment to 32.\n",
      "\n",
      "[2025-06-05 09:22:14] ubuntu:636:1071 [1] graph/tuning.cc:25 NCCL WARN Invalid NCCL_NTHREADS 32 (minimum 64).\n",
      "\n",
      "[2025-06-05 09:22:14] ubuntu:636:1071 [1] graph/tuning.cc:25 NCCL WARN Invalid NCCL_NTHREADS 32 (minimum 64).\n",
      "\n",
      "[2025-06-05 09:22:14] ubuntu:636:1071 [1] graph/tuning.cc:25 NCCL WARN Invalid NCCL_NTHREADS 32 (minimum 64).\n",
      "ubuntu:636:1071 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "ubuntu:636:1071 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\n",
      "ubuntu:636:1071 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.\n",
      "ubuntu:328:1070 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.\n",
      "ubuntu:636:1071 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.\n",
      "ubuntu:636:1071 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.\n",
      "ubuntu:328:1070 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.\n",
      "ubuntu:636:1071 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.\n",
      "ubuntu:328:1070 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.\n",
      "ubuntu:636:1071 [1] NCCL INFO ncclCommInitRankConfig comm 0x3da8e1d0 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 2a000 commId 0x56b91acfc7a7dbab - Init COMPLETE\n",
      "ubuntu:328:1070 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.\n",
      "ubuntu:636:1071 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.84 (kernels 0.65, alloc 0.09, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.06, rest 0.02)\n",
      "ubuntu:328:1070 [0] NCCL INFO ncclCommInitRankConfig comm 0x37a3d210 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 27000 commId 0x56b91acfc7a7dbab - Init COMPLETE\n",
      "ubuntu:328:1070 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 2 total 0.88 (kernels 0.14, alloc 0.09, bootstrap 0.54, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.07)\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:636:1078 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ubuntu:328:1079 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1\n",
      "ubuntu:636:1078 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1\n",
      "[NeMo I 2025-06-05 09:22:14 utils:554] Building Evo2Dataset splits with sizes=[400000, 8040, 2] and config=GPTDatasetConfig(random_seed=1234, sequence_length=1, blend=None, blend_per_split=[(['/workspace/preprocessed_data/XTT22_train_byte-level_train'], [1.0]), (['/workspace/preprocessed_data/XTT22_train_byte-level_val'], [1.0]), (['/workspace/preprocessed_data/XTT22_train_byte-level_test'], [1.0])], split=None, split_matrix=None, num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.bytelevel_tokenizers.ByteLevelTokenizer object at 0x7332dbee7740>, mid_level_dataset_surplus=0.005, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, object_storage_cache_path=None)\n",
      "[NeMo I 2025-06-05 09:22:14 utils:554] Load the _IndexReader from /workspace/preprocessed_data/XTT22_train_byte-level_train.idx\n",
      "[NeMo I 2025-06-05 09:22:14 utils:554] \tExtract the sequence lengths\n",
      "[NeMo I 2025-06-05 09:22:14 utils:554] \tExtract the sequence pointers\n",
      "[NeMo I 2025-06-05 09:22:14 utils:554] \tExtract the document indices\n",
      "[NeMo I 2025-06-05 09:22:14 utils:554] > total number of sequences: 10896\n",
      "[NeMo I 2025-06-05 09:22:14 utils:554] > total number of documents: 10896\n",
      "[NeMo I 2025-06-05 09:22:14 utils:554] Build and save the Evo2Dataset train indices\n",
      "[rank1]:[E605 09:32:14.239857255 ProcessGroupNCCL.cpp:633] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n",
      "[rank1]:[E605 09:32:14.240213264 ProcessGroupNCCL.cpp:757] [Rank 1] Work WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) timed out in blocking wait.\n",
      "ubuntu:636:1081 [1] NCCL INFO misc/socket.cc:64 -> 3\n",
      "ubuntu:636:1081 [1] NCCL INFO misc/socket.cc:80 -> 3\n",
      "ubuntu:636:1081 [1] NCCL INFO misc/socket.cc:829 -> 3\n",
      "ubuntu:636:1081 [1] NCCL INFO misc/socket.cc:64 -> 3\n",
      "ubuntu:636:1081 [1] NCCL INFO misc/socket.cc:80 -> 3\n",
      "ubuntu:636:1081 [1] NCCL INFO misc/socket.cc:829 -> 3\n",
      "ubuntu:636:1076 [1] NCCL INFO misc/socket.cc:881 -> 3\n",
      "ubuntu:636:1081 [1] NCCL INFO comm 0x3da8e1d0 rank 1 nranks 2 cudaDev 1 busId 2a000 - Abort COMPLETE\n",
      "[rank1]:[E605 09:32:15.765913519 ProcessGroupNCCL.cpp:685] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n",
      "[rank1]:[E605 09:32:15.765941872 ProcessGroupNCCL.cpp:699] [Rank 1] To avoid data inconsistency, we are taking the entire process down.\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/usr/local/bin/train_evo2\", line 10, in <module>\n",
      "[rank1]:     sys.exit(main())\n",
      "[rank1]:              ^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py\", line 1133, in main\n",
      "[rank1]:     train(args=args)\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py\", line 1126, in train\n",
      "[rank1]:     trainer.fit(model, data_module)\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\n",
      "[rank1]:     call._call_and_handle_interrupt(\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
      "[rank1]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
      "[rank1]:     return function(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\n",
      "[rank1]:     self._run(model, ckpt_path=ckpt_path)\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\", line 943, in _run\n",
      "[rank1]:     call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment\n",
      "[rank1]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\", line 102, in _call_setup_hook\n",
      "[rank1]:     _call_lightning_datamodule_hook(trainer, \"setup\", stage=fn)\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\", line 189, in _call_lightning_datamodule_hook\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/data/pre_training.py\", line 314, in setup\n",
      "[rank1]:     self.build(\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/data/pre_training.py\", line 304, in build\n",
      "[rank1]:     ).build()\n",
      "[rank1]:       ^^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 132, in build\n",
      "[rank1]:     datasets = self._build_blended_dataset_splits()\n",
      "[rank1]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 253, in _build_blended_dataset_splits\n",
      "[rank1]:     blended_datasets[i] = self._build_megatron_dataset_splits(\n",
      "[rank1]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 443, in _build_megatron_dataset_splits\n",
      "[rank1]:     self.build_generic_dataset(\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 507, in build_generic_dataset\n",
      "[rank1]:     torch.distributed.barrier()\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
      "[rank1]:     return func(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 4662, in barrier\n",
      "[rank1]:     work.wait()\n",
      "[rank1]: torch.distributed.DistBackendError: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n",
      "[rank: 1] Child process with PID 636 terminated with code 1. Forcefully terminating all other processes to avoid zombies 🧟\n"
     ]
    }
   ],
   "source": [
    "# ==================== NCCL超时问题解决方案 ====================\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. 备份和替换训练脚本\n",
    "print(\"🔧 备份并替换训练脚本...\")\n",
    "!cp /usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py /usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py.backup\n",
    "!cp /workspace/bionemo_train.py /usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py\n",
    "\n",
    "# 2. 设置NCCL和分布式环境变量\n",
    "print(\"🔧 配置NCCL超时和优化参数...\")\n",
    "\n",
    "# NCCL超时设置 - 增加到2小时\n",
    "os.environ['NCCL_TIMEOUT'] = '7200'  # 2小时超时\n",
    "os.environ['TORCH_NCCL_BLOCKING_WAIT'] = '1'  # 使用新的环境变量名\n",
    "os.environ['TORCH_NCCL_ASYNC_ERROR_HANDLING'] = '1'  # 使用新的环境变量名\n",
    "os.environ['NCCL_DEBUG'] = 'INFO'  # 启用详细调试信息\n",
    "\n",
    "# PyTorch分布式超时设置\n",
    "os.environ['TORCH_DISTRIBUTED_TIMEOUT'] = '7200'  # PyTorch分布式超时\n",
    "os.environ['TORCH_NCCL_TRACE_BUFFER_SIZE'] = '1024'  # 启用NCCL跟踪\n",
    "\n",
    "# 数据加载和通信优化\n",
    "os.environ['NCCL_BUFFSIZE'] = '8388608'  # 增加缓冲区大小到8MB\n",
    "os.environ['NCCL_NTHREADS'] = '8'  # 增加NCCL线程数\n",
    "os.environ['NCCL_MIN_NTHREADS'] = '4'  # 最小线程数\n",
    "\n",
    "# 避免内存碎片和并行冲突\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # 避免tokenizer并行冲突\n",
    "os.environ['OMP_NUM_THREADS'] = '4'  # 限制OpenMP线程数\n",
    "\n",
    "# 数据集准备优化\n",
    "os.environ['NCCL_P2P_DISABLE'] = '0'  # 确保P2P通信启用\n",
    "os.environ['NCCL_SHM_DISABLE'] = '0'  # 确保共享内存通信启用\n",
    "\n",
    "print(\"环境变量设置完成:\")\n",
    "for key in ['NCCL_TIMEOUT', 'TORCH_DISTRIBUTED_TIMEOUT', 'NCCL_DEBUG', 'NCCL_BUFFSIZE']:\n",
    "    print(f\"  {key}: {os.environ.get(key)}\")\n",
    "\n",
    "# 3. 定义带监控的训练函数\n",
    "def run_training_with_monitoring():\n",
    "    \"\"\"带实时监控的训练启动函数\"\"\"\n",
    "    \n",
    "    # 训练配置参数\n",
    "    training_config = {\n",
    "        'data_config': 'training_data_config.yaml',\n",
    "        'dataset_dir': 'preprocessed_data',\n",
    "        'model_size': '7b',\n",
    "        'devices': 2,\n",
    "        'num_nodes': 1,\n",
    "        'seq_length': 1,\n",
    "        'micro_batch_size': 1,\n",
    "        'lr': 0.0001,\n",
    "        'warmup_steps': 5,\n",
    "        'max_steps': 200000,\n",
    "        'ckpt_dir': 'nemo2_evo2_7b',\n",
    "        'clip_grad': 1,\n",
    "        'wd': 0.01,\n",
    "        'activation_checkpoint_recompute_num_layers': 1,\n",
    "        'val_check_interval': 1000\n",
    "    }\n",
    "    \n",
    "    # 构建训练命令\n",
    "    cmd = f\"\"\"\n",
    "    train_evo2 \\\\\n",
    "        -d {training_config['data_config']} \\\\\n",
    "        --dataset-dir {training_config['dataset_dir']} \\\\\n",
    "        --model-size {training_config['model_size']} \\\\\n",
    "        --devices {training_config['devices']} \\\\\n",
    "        --num-nodes {training_config['num_nodes']} \\\\\n",
    "        --seq-length {training_config['seq_length']} \\\\\n",
    "        --micro-batch-size {training_config['micro_batch_size']} \\\\\n",
    "        --lr {training_config['lr']} \\\\\n",
    "        --warmup-steps {training_config['warmup_steps']} \\\\\n",
    "        --max-steps {training_config['max_steps']} \\\\\n",
    "        --ckpt-dir {training_config['ckpt_dir']} \\\\\n",
    "        --clip-grad {training_config['clip_grad']} \\\\\n",
    "        --wd {training_config['wd']} \\\\\n",
    "        --activation-checkpoint-recompute-num-layers {training_config['activation_checkpoint_recompute_num_layers']} \\\\\n",
    "        --val-check-interval {training_config['val_check_interval']} \\\\\n",
    "        --ckpt-async-save\n",
    "    \"\"\".strip().replace('\\n', ' ').replace('  ', ' ')\n",
    "    \n",
    "    print(f\"🚀 开始训练时间: {datetime.now()}\")\n",
    "    print(f\"📋 训练命令: {cmd}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # 启动训练进程\n",
    "        process = subprocess.Popen(\n",
    "            cmd,\n",
    "            shell=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            universal_newlines=True,\n",
    "            bufsize=1\n",
    "        )\n",
    "        \n",
    "        # 实时监控输出\n",
    "        start_time = time.time()\n",
    "        last_output_time = start_time\n",
    "        dataset_preparation_detected = False\n",
    "        \n",
    "        for line in iter(process.stdout.readline, ''):\n",
    "            current_time = time.time()\n",
    "            elapsed = current_time - start_time\n",
    "            \n",
    "            # 打印带时间戳的输出\n",
    "            print(f\"[{elapsed:.1f}s] {line.rstrip()}\")\n",
    "            \n",
    "            # 检查关键信息\n",
    "            keywords = ['dataset', 'preparing', 'loading', 'barrier', 'build', 'index']\n",
    "            if any(keyword in line.lower() for keyword in keywords):\n",
    "                print(f\"📊 数据集准备阶段: {line.rstrip()}\")\n",
    "                dataset_preparation_detected = True\n",
    "                last_output_time = current_time\n",
    "            \n",
    "            # NCCL相关信息特别标记\n",
    "            if 'nccl' in line.lower():\n",
    "                print(f\"🔗 NCCL通信: {line.rstrip()}\")\n",
    "                last_output_time = current_time\n",
    "            \n",
    "            # 错误信息特别标记\n",
    "            if any(err in line.lower() for err in ['error', 'timeout', 'fail']):\n",
    "                print(f\"❌ 错误信息: {line.rstrip()}\")\n",
    "                last_output_time = current_time\n",
    "            \n",
    "            # 长时间无输出的警告\n",
    "            if current_time - last_output_time > 300:  # 5分钟无输出\n",
    "                elapsed_no_output = current_time - last_output_time\n",
    "                if dataset_preparation_detected:\n",
    "                    print(f\"\\n⏳ [数据集准备] 已有 {elapsed_no_output:.1f} 秒无输出，数据集构建中...\")\n",
    "                else:\n",
    "                    print(f\"\\n⏳ [等待中] 已有 {elapsed_no_output:.1f} 秒无输出...\")\n",
    "                last_output_time = current_time\n",
    "        \n",
    "        # 等待进程完成\n",
    "        return_code = process.wait()\n",
    "        \n",
    "        if return_code == 0:\n",
    "            print(f\"\\n✅ 训练成功完成! 总耗时: {time.time() - start_time:.1f} 秒\")\n",
    "        else:\n",
    "            print(f\"\\n❌ 训练失败，返回码: {return_code}\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n🛑 用户中断训练\")\n",
    "        process.terminate()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n💥 训练出错: {e}\")\n",
    "\n",
    "# 4. 启动训练\n",
    "print(\"🎯 启动带监控的训练...\")\n",
    "run_training_with_monitoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf40e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 备选方案：分步骤训练 ====================\n",
    "# 如果上面的方法仍然出现NCCL超时，可以尝试这个分步骤的方法\n",
    "\n",
    "def run_training_with_dataset_warmup():\n",
    "    \"\"\"分步骤训练：先单GPU预热数据集，再多GPU训练\"\"\"\n",
    "    \n",
    "    print(\"🔥 步骤1: 数据集预热（单GPU模式）\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 单GPU预热命令\n",
    "    warmup_cmd = \"\"\"\n",
    "    train_evo2 \\\\\n",
    "        -d training_data_config.yaml \\\\\n",
    "        --dataset-dir preprocessed_data \\\\\n",
    "        --model-size 7b \\\\\n",
    "        --devices 1 \\\\\n",
    "        --num-nodes 1 \\\\\n",
    "        --seq-length 1 \\\\\n",
    "        --micro-batch-size 1 \\\\\n",
    "        --max-steps 1 \\\\\n",
    "        --ckpt-dir warmup_ckpt \\\\\n",
    "        --val-check-interval 1\n",
    "    \"\"\".strip().replace('\\n', ' ').replace('  ', ' ')\n",
    "    \n",
    "    print(f\"预热命令: {warmup_cmd}\")\n",
    "    \n",
    "    # 执行预热\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            warmup_cmd, \n",
    "            shell=True, \n",
    "            capture_output=True, \n",
    "            text=True, \n",
    "            timeout=1800  # 30分钟超时\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"✅ 数据集预热成功!\")\n",
    "            print(\"🚀 步骤2: 开始正式多GPU训练\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # 清理预热的checkpoint\n",
    "            !rm -rf warmup_ckpt\n",
    "            \n",
    "            # 现在运行正常的多GPU训练\n",
    "            run_training_with_monitoring()\n",
    "        else:\n",
    "            print(f\"❌ 数据集预热失败:\")\n",
    "            print(f\"返回码: {result.returncode}\")\n",
    "            print(f\"错误输出: {result.stderr}\")\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"❌ 数据集预热超时（30分钟）\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 数据集预热出错: {e}\")\n",
    "\n",
    "# 取消注释下面的行来使用分步骤方法\n",
    "# print(\"🔄 启动分步骤训练...\")\n",
    "# run_training_with_dataset_warmup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 故障排除工具 ====================\n",
    "# 用于诊断NCCL和训练问题\n",
    "\n",
    "def diagnose_system():\n",
    "    \"\"\"系统诊断和故障排除\"\"\"\n",
    "    \n",
    "    print(\"🔍 系统诊断报告\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 检查GPU状态\n",
    "    print(\"📟 GPU状态:\")\n",
    "    !nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu --format=csv,noheader,nounits\n",
    "    \n",
    "    print(\"\\n💾 内存使用:\")\n",
    "    !free -h | head -2\n",
    "    \n",
    "    print(\"\\n💽 磁盘空间:\")\n",
    "    !df -h | grep -E '^/dev|File'\n",
    "    \n",
    "    print(\"\\n🔗 网络接口:\")\n",
    "    !ip addr show | grep -E '^[0-9]+:|inet '\n",
    "    \n",
    "    print(\"\\n🔧 NCCL环境变量:\")\n",
    "    !env | grep -E '^(NCCL|TORCH)' | sort\n",
    "    \n",
    "    print(\"\\n📊 进程状态:\")\n",
    "    !ps aux | grep -E '(train_evo2|python.*train)' | grep -v grep\n",
    "    \n",
    "    print(\"\\n🐍 PyTorch和CUDA版本:\")\n",
    "    import torch\n",
    "    print(f\"PyTorch版本: {torch.__version__}\")\n",
    "    print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
    "    print(f\"CUDA版本: {torch.version.cuda}\")\n",
    "    print(f\"CUDA设备数: {torch.cuda.device_count()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"    内存: {torch.cuda.get_device_properties(i).total_memory // 1024**3} GB\")\n",
    "\n",
    "def check_nccl_connectivity():\n",
    "    \"\"\"测试NCCL连接性\"\"\"\n",
    "    print(\"\\n🔗 NCCL连接性测试:\")\n",
    "    \n",
    "    test_script = '''\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() >= 2:\n",
    "    try:\n",
    "        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "        os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "        \n",
    "        # 测试NCCL初始化\n",
    "        dist.init_process_group(\"nccl\", rank=0, world_size=1)\n",
    "        print(\"✅ NCCL初始化成功\")\n",
    "        \n",
    "        # 测试tensor操作\n",
    "        tensor = torch.ones(2).cuda()\n",
    "        print(f\"✅ CUDA tensor创建成功: {tensor}\")\n",
    "        \n",
    "        dist.destroy_process_group()\n",
    "        print(\"✅ NCCL清理成功\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ NCCL测试失败: {e}\")\n",
    "else:\n",
    "    print(\"⚠️ 需要至少2个GPU来进行NCCL测试\")\n",
    "'''\n",
    "    \n",
    "    exec(test_script)\n",
    "\n",
    "# 运行诊断\n",
    "diagnose_system()\n",
    "check_nccl_connectivity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e795111f-d548-4d61-a1c1-85d6e95a2ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
