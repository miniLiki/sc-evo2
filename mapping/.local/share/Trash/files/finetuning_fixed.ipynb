{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "concat_path = \"XTT22_train.fa\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_fasta_path = os.path.abspath(concat_path)\n",
    "output_dir = os.path.abspath(\"preprocessed_data\")\n",
    "output_yaml = f\"\"\"\n",
    "- datapaths: [\"{full_fasta_path}\"]\n",
    "  output_dir: \"{output_dir}\"\n",
    "  output_prefix: XTT22_train\n",
    "  train_split: 0.9\n",
    "  valid_split: 0.05\n",
    "  test_split: 0.05\n",
    "  overwrite: True\n",
    "  embed_reverse_complement: true\n",
    "  random_reverse_complement: 0.0\n",
    "  random_lineage_dropout: 0.0\n",
    "  include_sequence_id: false\n",
    "  transcribe: \"back_transcribe\"\n",
    "  force_uppercase: false\n",
    "  indexed_dataset_dtype: \"uint8\"\n",
    "  tokenizer_type: \"Byte-Level\"\n",
    "  vocab_file: null\n",
    "  vocab_size: null\n",
    "  merges_file: null\n",
    "  pretrained_tokenizer_model: null\n",
    "  special_tokens: null\n",
    "  fast_hf_tokenizer: true\n",
    "  append_eod: true\n",
    "  enforce_sample_length: null\n",
    "  ftfy: false\n",
    "  workers: 1\n",
    "  preproc_concurrency: 100000\n",
    "  chunksize: 25\n",
    "  drop_empty_sequences: true\n",
    "  nnn_filter: false  # If you split your fasta on NNN (in human these are contigs), then you should set this to true.\n",
    "  seed: 12342  # Not relevant because we are not using random reverse complement or lineage dropout.\n",
    "\"\"\"\n",
    "with open(\"preprocess_config.yaml\", \"w\") as f:\n",
    "    print(output_yaml, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!preprocess_evo2 --config preprocess_config.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh preprocessed_data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /workspace/hyena_modified.py /usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/model/hyena.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda129.so')\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/evo2_convert_to_nemo2\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/bionemo/evo2/utils/checkpoint/convert_to_nemo.py\", line 173, in main\n",
      "    importer.apply(args.output_dir)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/model/hyena.py\", line 520, in apply\n",
      "    source = self.get_source_model()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/model/hyena.py\", line 507, in get_source_model\n",
      "    return torch.load(str(self), map_location='cpu')\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1496, in load\n",
      "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
      "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL _io.BytesIO was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_io.BytesIO])` or the `torch.serialization.safe_globals([_io.BytesIO])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
     ]
    }
   ],
   "source": [
    "!evo2_convert_to_nemo2 \\\n",
    "  --model-path /workspace/savanna_evo2_7b/savanna_evo2_7b.pt \\\n",
    "  --model-size 7b --output-dir nemo2_evo2_7b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 备份并替换训练脚本...\n",
      "🔧 配置NCCL超时和优化参数...\n",
      "环境变量设置完成:\n",
      "  NCCL_TIMEOUT: 7200\n",
      "  TORCH_DISTRIBUTED_TIMEOUT: 7200\n",
      "  NCCL_DEBUG: INFO\n",
      "  NCCL_BUFFSIZE: 8388608\n",
      "🎯 启动带监控的训练...\n",
      "📁 数据集目录: /workspace/preprocessed_data\n",
      "🚀 开始训练时间: 2025-06-05 12:03:51.918623\n",
      "📋 训练命令: train_evo2 -d training_data_config.yaml --dataset-dir /workspace/preprocessed_data --model-size 7b --devices 2 --num-nodes 1 --seq-length 1 --micro-batch-size 1 --lr 0.0001 --warmup-steps 5 --max-steps 200000 --ckpt-dir nemo2_evo2_7b --clip-grad 1 --wd 0.01 --activation-checkpoint-recompute-num-layers 1 --val-check-interval 1000 --ckpt-async-save\n",
      "================================================================================\n",
      "[12.2s] Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda129.so')\n",
      "[12.2s] The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[13.8s] 🔧 设置PyTorch分布式超时: 7200秒\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Using byte-level tokenization\n",
      "[13.8s] [NeMo W 2025-06-05 12:04:05 nemo_logging:405] WandB is currently turned off.\n",
      "[13.8s] [NeMo W 2025-06-05 12:04:05 nemo_logging:405] User-set tensorboard is currently turned off. Internally one may still be set by NeMo2.\n",
      "[13.8s] Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "[13.8s] GPU available: True (cuda), used: True\n",
      "[13.8s] TPU available: False, using: 0 TPU cores\n",
      "[13.8s] HPU available: False, using: 0 HPUs\n",
      "[13.8s] 启用 LoRA 微调...\n",
      "[13.8s] 模型结构调试信息:\n",
      "[13.8s] --------------------------------------------------\n",
      "[13.8s] 模型总共有 1 个模块\n",
      "[13.8s] 尝试访问模型的其他属性...\n",
      "[13.8s] 模块结构:\n",
      "[13.8s]    1.  (HyenaModel)\n",
      "[13.8s] \n",
      "[13.8s] 目标模式: ['module.decoder.layers.17.self_attention.linear_qkv']\n",
      "[13.8s] \n",
      "[13.8s] 所有线性层 (0):\n",
      "[13.8s] \n",
      "[13.8s] 找到的注意力相关层 (0):\n",
      "[13.8s] \n",
      "[13.8s] 找到的线性层 (0):\n",
      "[13.8s] \n",
      "[13.8s] 找到的QKV层 (0):\n",
      "[13.8s] ⚠️  没有找到任何线性层！模型可能还没有完全初始化。\n",
      "[13.8s] 这在 NeMo/Megatron 框架中是正常的，模型结构会在训练开始时初始化。\n",
      "[13.8s] LoRA 配置已保存，将在模型完全初始化后应用。\n",
      "[13.8s] 总参数数量: 0\n",
      "[13.8s] 可训练参数数量: 0\n",
      "[13.8s] ⚠️  警告: 模型总参数数量为0，可能模型初始化有问题\n",
      "[13.8s] ⚠️  警告: 没有可训练的参数，LoRA 可能没有正确应用\n",
      "[13.8s] LoRA 将在训练开始时自动应用\n",
      "[13.8s] 已添加 LoRA 回调，将在训练开始时应用 LoRA\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Experiments will be logged at results/evo2/dev\n",
      "[13.8s] [NeMo W 2025-06-05 12:04:05 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :results/evo2/checkpoints. Training from scratch.\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has data parallel group : [0, 1]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0, 1]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1]]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] All context parallel group ranks: [[0], [1]]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] All model parallel group ranks: [[0], [1]]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] All tensor model parallel group ranks: [[0], [1]]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1]]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] All embedding group ranks: [[0], [1]]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "[25.6s] Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda129.so')\n",
      "[25.6s] The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[27.5s] [W605 12:04:19.035675933 Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n",
      "🔗 NCCL通信: [W605 12:04:19.035675933 Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n",
      "[27.5s] [W605 12:04:19.035692497 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "🔗 NCCL通信: [W605 12:04:19.035692497 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "❌ 错误信息: [W605 12:04:19.035692497 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "[27.5s] [W605 12:04:19.046396689 Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n",
      "🔗 NCCL通信: [W605 12:04:19.046396689 Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n",
      "[27.5s] [W605 12:04:19.046433724 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "🔗 NCCL通信: [W605 12:04:19.046433724 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "❌ 错误信息: [W605 12:04:19.046433724 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "[27.5s] ----------------------------------------------------------------------------------------------------\n",
      "[27.5s] distributed_backend=nccl\n",
      "🔗 NCCL通信: distributed_backend=nccl\n",
      "[27.5s] All distributed processes registered. Starting with 2 processes\n",
      "[27.5s] ----------------------------------------------------------------------------------------------------\n",
      "[27.5s] \n",
      "[28.1s] ubuntu:2017:2017 [0] NCCL INFO Bootstrap: Using eth0:172.21.72.20<0>\n",
      "🔗 NCCL通信: ubuntu:2017:2017 [0] NCCL INFO Bootstrap: Using eth0:172.21.72.20<0>\n",
      "[28.1s] ubuntu:2017:2017 [0] NCCL INFO cudaDriverVersion 12090\n",
      "🔗 NCCL通信: ubuntu:2017:2017 [0] NCCL INFO cudaDriverVersion 12090\n",
      "[28.1s] ubuntu:2017:2017 [0] NCCL INFO NCCL version 2.26.3+cuda12.9\n",
      "🔗 NCCL通信: ubuntu:2017:2017 [0] NCCL INFO NCCL version 2.26.3+cuda12.9\n",
      "[28.1s] ubuntu:2017:2017 [0] NCCL INFO Comm config Blocking set to 1\n",
      "🔗 NCCL通信: ubuntu:2017:2017 [0] NCCL INFO Comm config Blocking set to 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v10 symbol.\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v10 symbol.\n",
      "❌ 错误信息: ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v10 symbol.\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v9 (v9)\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v9 (v9)\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.\n",
      "❌ 错误信息: ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Loaded collnet plugin SHARP (v9)\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Loaded collnet plugin SHARP (v9)\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO P2P plugin v9 IBext_v9\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO P2P plugin v9 IBext_v9\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/IB : No device found.\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO NET/IB : No device found.\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/IB : No device found.\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO NET/IB : No device found.\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/Socket : Using [0]eth0:172.21.72.20<0> [1]eth1:172.25.58.75<0> [2]nodelocaldns:169.254.25.10<0> [3]vxlan.calico:10.233.112.0<0> [4]cali829ebf906e6:fe80::ecee:eeff:feee:eeee%cali829ebf906e6<0> [5]cali9ffd33e0038:fe80::ecee:eeff:feee:eeee%cali9ffd33e0038<0> [6]cali210c644397a:fe80::ecee:eeff:feee:eeee%cali210c644397a<0> [7]caliafd05346b22:fe80::ecee:eeff:feee:eeee%caliafd05346b22<0> [8]calicd149c44f0f:fe80::ecee:eeff:feee:eeee%calicd149c44f0f<0> [9]cali62597977d84:fe80::ecee:eeff:feee:eeee%cali62597977d84<0> [10]calidee47cce1bb:fe80::ecee:eeff:feee:eeee%calidee47cce1bb<0> [11]cali2ad9843228b:fe80::ecee:eeff:feee:eeee%cali2ad9843228b<0> [12]calicb98c4ca0f1:fe80::ecee:eeff:feee:eeee%calicb98c4ca0f1<0> [13]cali9c537539f6f:fe80::ecee:eeff:feee:eeee%cali9c537539f6f<0> [14]cali3dd684f6336:fe80::ecee:eeff:feee:eeee%cali3dd684f6336<0> [15]calid294f721236:fe80::ecee:eeff:feee:eeee%calid294f721236<0>\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO NET/Socket : Using [0]eth0:172.21.72.20<0> [1]eth1:172.25.58.75<0> [2]nodelocaldns:169.254.25.10<0> [3]vxlan.calico:10.233.112.0<0> [4]cali829ebf906e6:fe80::ecee:eeff:feee:eeee%cali829ebf906e6<0> [5]cali9ffd33e0038:fe80::ecee:eeff:feee:eeee%cali9ffd33e0038<0> [6]cali210c644397a:fe80::ecee:eeff:feee:eeee%cali210c644397a<0> [7]caliafd05346b22:fe80::ecee:eeff:feee:eeee%caliafd05346b22<0> [8]calicd149c44f0f:fe80::ecee:eeff:feee:eeee%calicd149c44f0f<0> [9]cali62597977d84:fe80::ecee:eeff:feee:eeee%cali62597977d84<0> [10]calidee47cce1bb:fe80::ecee:eeff:feee:eeee%calidee47cce1bb<0> [11]cali2ad9843228b:fe80::ecee:eeff:feee:eeee%cali2ad9843228b<0> [12]calicb98c4ca0f1:fe80::ecee:eeff:feee:eeee%calicb98c4ca0f1<0> [13]cali9c537539f6f:fe80::ecee:eeff:feee:eeee%cali9c537539f6f<0> [14]cali3dd684f6336:fe80::ecee:eeff:feee:eeee%cali3dd684f6336<0> [15]calid294f721236:fe80::ecee:eeff:feee:eeee%calid294f721236<0>\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Using network Socket\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Using network Socket\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO ncclCommInitRankConfig comm 0x459f9440 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 27000 commId 0x8c56ad3ef59f2eb8 - Init START\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO ncclCommInitRankConfig comm 0x459f9440 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 27000 commId 0x8c56ad3ef59f2eb8 - Init START\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Bootstrap timings total 0.020981 (create 0.000034, send 0.016701, recv 0.003764, ring 0.000058, delay 0.000001)\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Bootstrap timings total 0.020981 (create 0.000034, send 0.016701, recv 0.003764, ring 0.000058, delay 0.000001)\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO comm 0x459f9440 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO comm 0x459f9440 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 00/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 00/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 01/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 01/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 02/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 02/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 03/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 03/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 04/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 04/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 05/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 05/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 06/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 06/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 07/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 07/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 08/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 08/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 09/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 09/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 10/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 10/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 11/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 11/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 12/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 12/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 13/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 13/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 14/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 14/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 15/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 15/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 16/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 16/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 17/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 17/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 18/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 18/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 19/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 19/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 20/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 20/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 21/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 21/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 22/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 22/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 23/24 : 0 1\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Channel 23/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] -1/-1/-1->0->1 [9] -1/-1/-1->0->1 ubuntu:2110:2110 [1] NCCL INFO cudaDriverVersion 12090\n",
      "🔗 NCCL通信: ubuntu:2017:2226 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] -1/-1/-1->0->1 [9] -1/-1/-1->0->1 ubuntu:2110:2110 [1] NCCL INFO cudaDriverVersion 12090\n",
      "[28.1s] ubuntu:2110:2110 [1] NCCL INFO Bootstrap: Using eth0:172.21.72.20<0>\n",
      "🔗 NCCL通信: ubuntu:2110:2110 [1] NCCL INFO Bootstrap: Using eth0:172.21.72.20<0>\n",
      "[28.1s] ubuntu:2110:2110 [1] NCCL INFO NCCL version 2.26.3+cuda12.9\n",
      "🔗 NCCL通信: ubuntu:2110:2110 [1] NCCL INFO NCCL version 2.26.3+cuda12.9\n",
      "[28.1s] ubuntu:2110:2110 [1] NCCL INFO Comm config Blocking set to 1\n",
      "🔗 NCCL通信: ubuntu:2110:2110 [1] NCCL INFO Comm config Blocking set to 1\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v10 symbol.\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v10 symbol.\n",
      "❌ 错误信息: ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v10 symbol.\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v9 (v9)\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v9 (v9)\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.\n",
      "❌ 错误信息: ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Loaded collnet plugin SHARP (v9)\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Loaded collnet plugin SHARP (v9)\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO P2P plugin v9 IBext_v9\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO P2P plugin v9 IBext_v9\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/IB : No device found.\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO NET/IB : No device found.\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/IB : No device found.\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO NET/IB : No device found.\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/Socket : Using [0]eth0:172.21.72.20<0> [1]eth1:172.25.58.75<0> [2]nodelocaldns:169.254.25.10<0> [3]vxlan.calico:10.233.112.0<0> [4]cali829ebf906e6:fe80::ecee:eeff:feee:eeee%cali829ebf906e6<0> [5]cali9ffd33e0038:fe80::ecee:eeff:feee:eeee%cali9ffd33e0038<0> [6]cali210c644397a:fe80::ecee:eeff:feee:eeee%cali210c644397a<0> [7]caliafd05346b22:fe80::ecee:eeff:feee:eeee%caliafd05346b22<0> [8]calicd149c44f0f:fe80::ecee:eeff:feee:eeee%calicd149c44f0f<0> [9]cali62597977d84:fe80::ecee:eeff:feee:eeee%cali62597977d84<0> [10]calidee47cce1bb:fe80::ecee:eeff:feee:eeee%calidee47cce1bb<0> [11]cali2ad9843228b:fe80::ecee:eeff:feee:eeee%cali2ad9843228b<0> [12]calicb98c4ca0f1:fe80::ecee:eeff:feee:eeee%calicb98c4ca0f1<0> [13]cali9c537539f6f:fe80::ecee:eeff:feee:eeee%cali9c537539f6f<0> [14]cali3dd684f6336:fe80::ecee:eeff:feee:eeee%cali3dd684f6336<0> [15]calid294f721236:fe80::ecee:eeff:feee:eeee%calid294f721236<0>\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO NET/Socket : Using [0]eth0:172.21.72.20<0> [1]eth1:172.25.58.75<0> [2]nodelocaldns:169.254.25.10<0> [3]vxlan.calico:10.233.112.0<0> [4]cali829ebf906e6:fe80::ecee:eeff:feee:eeee%cali829ebf906e6<0> [5]cali9ffd33e0038:fe80::ecee:eeff:feee:eeee%cali9ffd33e0038<0> [6]cali210c644397a:fe80::ecee:eeff:feee:eeee%cali210c644397a<0> [7]caliafd05346b22:fe80::ecee:eeff:feee:eeee%caliafd05346b22<0> [8]calicd149c44f0f:fe80::ecee:eeff:feee:eeee%calicd149c44f0f<0> [9]cali62597977d84:fe80::ecee:eeff:feee:eeee%cali62597977d84<0> [10]calidee47cce1bb:fe80::ecee:eeff:feee:eeee%calidee47cce1bb<0> [11]cali2ad9843228b:fe80::ecee:eeff:feee:eeee%cali2ad9843228b<0> [12]calicb98c4ca0f1:fe80::ecee:eeff:feee:eeee%calicb98c4ca0f1<0> [13]cali9c537539f6f:fe80::ecee:eeff:feee:eeee%cali9c537539f6f<0> [14]cali3dd684f6336:fe80::ecee:eeff:feee:eeee%cali3dd684f6336<0> [15]calid294f721236:fe80::ecee:eeff:feee:eeee%calid294f721236<0>\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO Using network Socket\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO Using network Socket\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO ncclCommInitRankConfig comm 0x22d88b30 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 2a000 commId 0x8c56ad3ef59f2eb8 - Init START\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO ncclCommInitRankConfig comm 0x22d88b30 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 2a000 commId 0x8c56ad3ef59f2eb8 - Init START\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO RAS client listening socket at 127.0.0.1<28028>\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO RAS client listening socket at 127.0.0.1<28028>\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO Bootstrap timings total 0.000840 (create 0.000033, send 0.000181, recv 0.000228, ring 0.000040, delay 0.000001)\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO Bootstrap timings total 0.000840 (create 0.000033, send 0.000181, recv 0.000228, ring 0.000040, delay 0.000001)\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO comm 0x22d88b30 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO comm 0x22d88b30 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] 0/-1/-1->1->-1 [7] 0/-1/-1->1->-1 [8] 0/-1/-1->1->-1 [9] 0/-1/-1->1->-1 [10] 0/-1/-1->1->-1 [11] 0/-1/-1->1->-1 [12] -1/-1/-1->1->0 [13] -1/-1/-1->1->0 [14] -1/-1/-1->1->0 [15] -1/-1/-1->1->0 [16] -1/-1/-1->1->0 [17] -1/-1/-1->1->0 [18] 0/-1/-1->1->-1 [19] 0/-1/-1->1->-1 [20] 0/-1/-1->1->-1 [21] 0/-1/-1->1->-1 [22] 0/-1/-1->1->-1 [23] 0/-1/-1->1->-1\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] 0/-1/-1->1->-1 [7] 0/-1/-1->1->-1 [8] 0/-1/-1->1->-1 [9] 0/-1/-1->1->-1 [10] 0/-1/-1->1->-1 [11] 0/-1/-1->1->-1 [12] -1/-1/-1->1->0 [13] -1/-1/-1->1->0 [14] -1/-1/-1->1->0 [15] -1/-1/-1->1->0 [16] -1/-1/-1->1->0 [17] -1/-1/-1->1->0 [18] 0/-1/-1->1->-1 [19] 0/-1/-1->1->-1 [20] 0/-1/-1->1->-1 [21] 0/-1/-1->1->-1 [22] 0/-1/-1->1->-1 [23] 0/-1/-1->1->-1\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NCCL_BUFFSIZE set by environment to 8388608.\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO NCCL_BUFFSIZE set by environment to 8388608.\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO P2P Chunksize set to 524288\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO P2P Chunksize set to 524288\n",
      "[28.1s] ubuntu:2110:2232 [1] NCCL INFO [Proxy Service] Device 1 CPU core 71\n",
      "🔗 NCCL通信: ubuntu:2110:2232 [1] NCCL INFO [Proxy Service] Device 1 CPU core 71\n",
      "[28.1s] ubuntu:2110:2233 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 25\n",
      "🔗 NCCL通信: ubuntu:2110:2233 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 25\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NCCL_NTHREADS set by environment to 8.\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO NCCL_NTHREADS set by environment to 8.\n",
      "[28.1s] \n",
      "[28.1s] [2025-06-05 12:04:20] ubuntu:2110:2227 [1] graph/tuning.cc:19 NCCL WARN Invalid NCCL_NTHREADS 8 (must be a multiple of 32)\n",
      "🔗 NCCL通信: [2025-06-05 12:04:20] ubuntu:2110:2227 [1] graph/tuning.cc:19 NCCL WARN Invalid NCCL_NTHREADS 8 (must be a multiple of 32)\n",
      "[28.1s] \n",
      "[28.1s] [2025-06-05 12:04:20] ubuntu:2110:2227 [1] graph/tuning.cc:19 NCCL WARN Invalid NCCL_NTHREADS 8 (must be a multiple of 32)\n",
      "🔗 NCCL通信: [2025-06-05 12:04:20] ubuntu:2110:2227 [1] graph/tuning.cc:19 NCCL WARN Invalid NCCL_NTHREADS 8 (must be a multiple of 32)\n",
      "[28.1s] \n",
      "[28.1s] [2025-06-05 12:04:20] ubuntu:2110:2227 [1] graph/tuning.cc:19 NCCL WARN Invalid NCCL_NTHREADS 8 (must be a multiple of 32)\n",
      "🔗 NCCL通信: [2025-06-05 12:04:20] ubuntu:2110:2227 [1] graph/tuning.cc:19 NCCL WARN Invalid NCCL_NTHREADS 8 (must be a multiple of 32)\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\n",
      "[28.5s] ubuntu:2110:2227 [1] [NeMo I 2025-06-05 12:04:20 utils:554] Building Evo2Dataset splits with sizes=[400000, 8040, 2] and config=GPTDatasetConfig(random_seed=1234, sequence_length=1, blend=None, blend_per_split=[(['/workspace/preprocessed_data/XTT22_train_byte-level_train'], [1.0]), (['/workspace/preprocessed_data/XTT22_train_byte-level_val'], [1.0]), (['/workspace/preprocessed_data/XTT22_train_byte-level_test'], [1.0])], split=None, split_matrix=None, num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.bytelevel_tokenizers.ByteLevelTokenizer object at 0x727accc3c1d0>, mid_level_dataset_surplus=0.005, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, object_storage_cache_path=None)\n",
      "📊 数据集准备阶段: ubuntu:2110:2227 [1] [NeMo I 2025-06-05 12:04:20 utils:554] Building Evo2Dataset splits with sizes=[400000, 8040, 2] and config=GPTDatasetConfig(random_seed=1234, sequence_length=1, blend=None, blend_per_split=[(['/workspace/preprocessed_data/XTT22_train_byte-level_train'], [1.0]), (['/workspace/preprocessed_data/XTT22_train_byte-level_val'], [1.0]), (['/workspace/preprocessed_data/XTT22_train_byte-level_test'], [1.0])], split=None, split_matrix=None, num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.bytelevel_tokenizers.ByteLevelTokenizer object at 0x727accc3c1d0>, mid_level_dataset_surplus=0.005, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, object_storage_cache_path=None)\n",
      "[28.5s] [NeMo I 2025-06-05 12:04:20 utils:554] Load the _IndexReader from /workspace/preprocessed_data/XTT22_train_byte-level_train.idx\n",
      "📊 数据集准备阶段: [NeMo I 2025-06-05 12:04:20 utils:554] Load the _IndexReader from /workspace/preprocessed_data/XTT22_train_byte-level_train.idx\n",
      "[28.5s] [NeMo I 2025-06-05 12:04:20 utils:554] \tExtract the sequence lengths\n",
      "[28.5s] [NeMo I 2025-06-05 12:04:20 utils:554] \tExtract the sequence pointers\n",
      "[28.5s] [NeMo I 2025-06-05 12:04:20 utils:554] \tExtract the document indices\n",
      "[28.5s] [NeMo I 2025-06-05 12:04:20 utils:554] > total number of sequences: 10896\n",
      "[28.5s] [NeMo I 2025-06-05 12:04:20 utils:554] > total number of documents: 10896\n",
      "[28.5s] [NeMo I 2025-06-05 12:04:20 utils:554] Build and save the Evo2Dataset train indices\n",
      "📊 数据集准备阶段: [NeMo I 2025-06-05 12:04:20 utils:554] Build and save the Evo2Dataset train indices\n",
      "[628.5s] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.\n",
      "🔗 NCCL通信: NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.\n",
      "[628.5s] ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.\n",
      "❌ 错误信息: ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.\n",
      "[628.5s] ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.\n",
      "❌ 错误信息: ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.\n",
      "[628.5s] ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.\n",
      "❌ 错误信息: ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.\n",
      "[628.5s] ubuntu:2110:2227 [1] NCCL INFO ncclCommInitRankConfig comm 0x22d88b30 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 2a000 commId 0x8c56ad3ef59f2eb8 - Init COMPLETE\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO ncclCommInitRankConfig comm 0x22d88b30 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 2a000 commId 0x8c56ad3ef59f2eb8 - Init COMPLETE\n",
      "[628.5s] ubuntu:2110:2227 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.26 (kernels 0.13, alloc 0.09, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.01, rest 0.01)\n",
      "🔗 NCCL通信: ubuntu:2110:2227 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.26 (kernels 0.13, alloc 0.09, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.01, rest 0.01)\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1\n",
      "🔗 NCCL通信: ubuntu:2110:2235 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1\n",
      "[628.5s] [rank1]:[E605 12:14:20.023183723 ProcessGroupNCCL.cpp:633] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n",
      "🔗 NCCL通信: [rank1]:[E605 12:14:20.023183723 ProcessGroupNCCL.cpp:633] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n",
      "❌ 错误信息: [rank1]:[E605 12:14:20.023183723 ProcessGroupNCCL.cpp:633] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n",
      "[628.5s] [rank1]:[E605 12:14:20.023544412 ProcessGroupNCCL.cpp:757] [Rank 1] Work WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) timed out in blocking wait.\n",
      "🔗 NCCL通信: [rank1]:[E605 12:14:20.023544412 ProcessGroupNCCL.cpp:757] [Rank 1] Work WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) timed out in blocking wait.\n",
      "❌ 错误信息: [rank1]:[E605 12:14:20.023544412 ProcessGroupNCCL.cpp:757] [Rank 1] Work WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) timed out in blocking wait.\n",
      "[629.0s] ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:64 -> 3\n",
      "🔗 NCCL通信: ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:64 -> 3\n",
      "[629.0s] ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:80 -> 3\n",
      "🔗 NCCL通信: ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:80 -> 3\n",
      "[629.0s] ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:829 -> 3\n",
      "🔗 NCCL通信: ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:829 -> 3\n",
      "[629.0s] ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:64 -> 3\n",
      "🔗 NCCL通信: ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:64 -> 3\n",
      "[629.0s] ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:80 -> 3\n",
      "🔗 NCCL通信: ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:80 -> 3\n",
      "[629.0s] ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:829 -> 3\n",
      "🔗 NCCL通信: ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:829 -> 3\n",
      "[629.0s] ubuntu:2110:2232 [1] NCCL INFO misc/socket.cc:881 -> 3\n",
      "🔗 NCCL通信: ubuntu:2110:2232 [1] NCCL INFO misc/socket.cc:881 -> 3\n",
      "[629.0s] ubuntu:2110:2237 [1] NCCL INFO comm 0x22d88b30 rank 1 nranks 2 cudaDev 1 busId 2a000 - Abort COMPLETE\n",
      "🔗 NCCL通信: ubuntu:2110:2237 [1] NCCL INFO comm 0x22d88b30 rank 1 nranks 2 cudaDev 1 busId 2a000 - Abort COMPLETE\n",
      "[629.0s] [rank1]:[E605 12:14:20.550697962 ProcessGroupNCCL.cpp:685] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n",
      "🔗 NCCL通信: [rank1]:[E605 12:14:20.550697962 ProcessGroupNCCL.cpp:685] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n",
      "❌ 错误信息: [rank1]:[E605 12:14:20.550697962 ProcessGroupNCCL.cpp:685] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n",
      "[629.0s] [rank1]:[E605 12:14:20.550744322 ProcessGroupNCCL.cpp:699] [Rank 1] To avoid data inconsistency, we are taking the entire process down.\n",
      "🔗 NCCL通信: [rank1]:[E605 12:14:20.550744322 ProcessGroupNCCL.cpp:699] [Rank 1] To avoid data inconsistency, we are taking the entire process down.\n",
      "[629.0s] 🔧 设置PyTorch分布式超时: 7200秒\n",
      "[629.0s] 启用 LoRA 微调...\n",
      "[629.0s] 模型结构调试信息:\n",
      "[629.0s] --------------------------------------------------\n",
      "[629.0s] 模型总共有 1 个模块\n",
      "[629.0s] 尝试访问模型的其他属性...\n",
      "[629.0s] 模块结构:\n",
      "[629.0s]    1.  (HyenaModel)\n",
      "[629.0s] \n",
      "[629.0s] 目标模式: ['module.decoder.layers.17.self_attention.linear_qkv']\n",
      "[629.0s] \n",
      "[629.0s] 所有线性层 (0):\n",
      "[629.0s] \n",
      "[629.0s] 找到的注意力相关层 (0):\n",
      "[629.0s] \n",
      "[629.0s] 找到的线性层 (0):\n",
      "[629.0s] \n",
      "[629.0s] 找到的QKV层 (0):\n",
      "[629.0s] ⚠️  没有找到任何线性层！模型可能还没有完全初始化。\n",
      "[629.0s] 这在 NeMo/Megatron 框架中是正常的，模型结构会在训练开始时初始化。\n",
      "[629.0s] LoRA 配置已保存，将在模型完全初始化后应用。\n",
      "[629.0s] 总参数数量: 0\n",
      "[629.0s] 可训练参数数量: 0\n",
      "[629.0s] ⚠️  警告: 模型总参数数量为0，可能模型初始化有问题\n",
      "[629.0s] ⚠️  警告: 没有可训练的参数，LoRA 可能没有正确应用\n",
      "[629.0s] LoRA 将在训练开始时自动应用\n",
      "[629.0s] 已添加 LoRA 回调，将在训练开始时应用 LoRA\n",
      "[629.0s] [rank1]: Traceback (most recent call last):\n",
      "[629.0s] [rank1]:   File \"/usr/local/bin/train_evo2\", line 10, in <module>\n",
      "[629.0s] [rank1]:     sys.exit(main())\n",
      "[629.0s] [rank1]:              ^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py\", line 1133, in main\n",
      "[629.0s] [rank1]:     train(args=args)\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py\", line 1126, in train\n",
      "[629.0s] [rank1]:     trainer.fit(model, data_module)\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\n",
      "[629.0s] [rank1]:     call._call_and_handle_interrupt(\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
      "[629.0s] [rank1]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "[629.0s] [rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
      "[629.0s] [rank1]:     return function(*args, **kwargs)\n",
      "[629.0s] [rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\n",
      "[629.0s] [rank1]:     self._run(model, ckpt_path=ckpt_path)\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\", line 943, in _run\n",
      "[629.0s] [rank1]:     call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment\n",
      "[629.0s] [rank1]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\", line 102, in _call_setup_hook\n",
      "[629.0s] [rank1]:     _call_lightning_datamodule_hook(trainer, \"setup\", stage=fn)\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\", line 189, in _call_lightning_datamodule_hook\n",
      "[629.0s] [rank1]:     return fn(*args, **kwargs)\n",
      "[629.0s] [rank1]:            ^^^^^^^^^^^^^^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/data/pre_training.py\", line 314, in setup\n",
      "[629.0s] [rank1]:     self.build(\n",
      "📊 数据集准备阶段: [rank1]:     self.build(\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/data/pre_training.py\", line 304, in build\n",
      "📊 数据集准备阶段: [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/data/pre_training.py\", line 304, in build\n",
      "[629.0s] [rank1]:     ).build()\n",
      "📊 数据集准备阶段: [rank1]:     ).build()\n",
      "[629.0s] [rank1]:       ^^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 132, in build\n",
      "📊 数据集准备阶段: [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 132, in build\n",
      "[629.0s] [rank1]:     datasets = self._build_blended_dataset_splits()\n",
      "📊 数据集准备阶段: [rank1]:     datasets = self._build_blended_dataset_splits()\n",
      "[629.0s] [rank1]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 253, in _build_blended_dataset_splits\n",
      "📊 数据集准备阶段: [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 253, in _build_blended_dataset_splits\n",
      "[629.0s] [rank1]:     blended_datasets[i] = self._build_megatron_dataset_splits(\n",
      "📊 数据集准备阶段: [rank1]:     blended_datasets[i] = self._build_megatron_dataset_splits(\n",
      "[629.0s] [rank1]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 443, in _build_megatron_dataset_splits\n",
      "📊 数据集准备阶段: [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 443, in _build_megatron_dataset_splits\n",
      "[629.0s] [rank1]:     self.build_generic_dataset(\n",
      "📊 数据集准备阶段: [rank1]:     self.build_generic_dataset(\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 507, in build_generic_dataset\n",
      "📊 数据集准备阶段: [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 507, in build_generic_dataset\n",
      "[629.0s] [rank1]:     torch.distributed.barrier()\n",
      "📊 数据集准备阶段: [rank1]:     torch.distributed.barrier()\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
      "[629.0s] [rank1]:     return func(*args, **kwargs)\n",
      "[629.0s] [rank1]:            ^^^^^^^^^^^^^^^^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 4662, in barrier\n",
      "📊 数据集准备阶段: [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 4662, in barrier\n",
      "[629.0s] [rank1]:     work.wait()\n",
      "[629.0s] [rank1]: torch.distributed.DistBackendError: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n",
      "🔗 NCCL通信: [rank1]: torch.distributed.DistBackendError: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n",
      "❌ 错误信息: [rank1]: torch.distributed.DistBackendError: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n",
      "[634.3s] [rank: 1] Child process with PID 2110 terminated with code 1. Forcefully terminating all other processes to avoid zombies 🧟\n",
      "\n",
      "❌ 训练失败，返回码: -9\n"
     ]
    }
   ],
   "source": [
    "# ==================== NCCL超时问题解决方案 ====================\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. 备份和替换训练脚本\n",
    "print(\"🔧 备份并替换训练脚本...\")\n",
    "!cp /usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py /usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py.backup 2>/dev/null || echo \"备份文件已存在或路径不存在\"\n",
    "!cp /workspace/bionemo_train.py /usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py 2>/dev/null || echo \"自定义训练脚本不存在，使用默认版本\"\n",
    "\n",
    "# 2. 设置NCCL和分布式环境变量\n",
    "print(\"🔧 配置NCCL超时和优化参数...\")\n",
    "\n",
    "# NCCL超时设置 - 增加到2小时\n",
    "os.environ['NCCL_TIMEOUT'] = '7200'  # 2小时超时\n",
    "os.environ['TORCH_NCCL_BLOCKING_WAIT'] = '1'  # 使用新的环境变量名\n",
    "os.environ['TORCH_NCCL_ASYNC_ERROR_HANDLING'] = '1'  # 使用新的环境变量名\n",
    "os.environ['NCCL_DEBUG'] = 'INFO'  # 启用详细调试信息\n",
    "\n",
    "# PyTorch分布式超时设置\n",
    "os.environ['TORCH_DISTRIBUTED_TIMEOUT'] = '7200'  # PyTorch分布式超时\n",
    "os.environ['TORCH_NCCL_TRACE_BUFFER_SIZE'] = '1024'  # 启用NCCL跟踪\n",
    "\n",
    "# 数据加载和通信优化\n",
    "os.environ['NCCL_BUFFSIZE'] = '8388608'  # 增加缓冲区大小到8MB\n",
    "os.environ['NCCL_NTHREADS'] = '8'  # 增加NCCL线程数\n",
    "os.environ['NCCL_MIN_NTHREADS'] = '4'  # 最小线程数\n",
    "\n",
    "# 避免内存碎片和并行冲突\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # 避免tokenizer并行冲突\n",
    "os.environ['OMP_NUM_THREADS'] = '4'  # 限制OpenMP线程数\n",
    "\n",
    "# 数据集准备优化\n",
    "os.environ['NCCL_P2P_DISABLE'] = '0'  # 确保P2P通信启用\n",
    "os.environ['NCCL_SHM_DISABLE'] = '0'  # 确保共享内存通信启用\n",
    "\n",
    "print(\"环境变量设置完成:\")\n",
    "for key in ['NCCL_TIMEOUT', 'TORCH_DISTRIBUTED_TIMEOUT', 'NCCL_DEBUG', 'NCCL_BUFFSIZE']:\n",
    "    print(f\"  {key}: {os.environ.get(key)}\")\n",
    "\n",
    "# 3. 定义带监控的训练函数\n",
    "def run_training_with_monitoring():\n",
    "    \"\"\"带实时监控的训练启动函数\"\"\"\n",
    "    \n",
    "    # 获取当前工作目录中的preprocessed_data路径\n",
    "    preprocessed_data = os.path.abspath(\"preprocessed_data\")\n",
    "    print(f\"📁 数据集目录: {preprocessed_data}\")\n",
    "    \n",
    "    # 训练配置参数\n",
    "    training_config = {\n",
    "        'data_config': 'training_data_config.yaml',\n",
    "        'dataset_dir': preprocessed_data,  # 使用实际路径\n",
    "        'model_size': '7b',\n",
    "        'devices': 2,\n",
    "        'num_nodes': 1,\n",
    "        'seq_length': 1,\n",
    "        'micro_batch_size': 1,\n",
    "        'lr': 0.0001,\n",
    "        'warmup_steps': 5,\n",
    "        'max_steps': 200000,\n",
    "        'ckpt_dir': 'nemo2_evo2_7b',\n",
    "        'clip_grad': 1,\n",
    "        'wd': 0.01,\n",
    "        'activation_checkpoint_recompute_num_layers': 1,\n",
    "        'val_check_interval': 1000\n",
    "    }\n",
    "    \n",
    "    # 构建训练命令 - 使用正确的格式\n",
    "    cmd_parts = [\n",
    "        'train_evo2',\n",
    "        '-d', training_config['data_config'],\n",
    "        '--dataset-dir', training_config['dataset_dir'],\n",
    "        '--model-size', training_config['model_size'],\n",
    "        '--devices', str(training_config['devices']),\n",
    "        '--num-nodes', str(training_config['num_nodes']),\n",
    "        '--seq-length', str(training_config['seq_length']),\n",
    "        '--micro-batch-size', str(training_config['micro_batch_size']),\n",
    "        '--lr', str(training_config['lr']),\n",
    "        '--warmup-steps', str(training_config['warmup_steps']),\n",
    "        '--max-steps', str(training_config['max_steps']),\n",
    "        '--ckpt-dir', training_config['ckpt_dir'],\n",
    "        '--clip-grad', str(training_config['clip_grad']),\n",
    "        '--wd', str(training_config['wd']),\n",
    "        '--activation-checkpoint-recompute-num-layers', str(training_config['activation_checkpoint_recompute_num_layers']),\n",
    "        '--val-check-interval', str(training_config['val_check_interval']),\n",
    "        '--ckpt-async-save'\n",
    "    ]\n",
    "    \n",
    "    cmd = ' '.join(cmd_parts)\n",
    "    \n",
    "    print(f\"🚀 开始训练时间: {datetime.now()}\")\n",
    "    print(f\"📋 训练命令: {cmd}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # 启动训练进程\n",
    "        process = subprocess.Popen(\n",
    "            cmd_parts,  # 使用列表而不是字符串，避免shell解析问题\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            universal_newlines=True,\n",
    "            bufsize=1\n",
    "        )\n",
    "        \n",
    "        # 实时监控输出\n",
    "        start_time = time.time()\n",
    "        last_output_time = start_time\n",
    "        dataset_preparation_detected = False\n",
    "        \n",
    "        for line in iter(process.stdout.readline, ''):\n",
    "            current_time = time.time()\n",
    "            elapsed = current_time - start_time\n",
    "            \n",
    "            # 打印带时间戳的输出\n",
    "            print(f\"[{elapsed:.1f}s] {line.rstrip()}\")\n",
    "            \n",
    "            # 检查关键信息\n",
    "            keywords = ['dataset', 'preparing', 'loading', 'barrier', 'build', 'index']\n",
    "            if any(keyword in line.lower() for keyword in keywords):\n",
    "                print(f\"📊 数据集准备阶段: {line.rstrip()}\")\n",
    "                dataset_preparation_detected = True\n",
    "                last_output_time = current_time\n",
    "            \n",
    "            # NCCL相关信息特别标记\n",
    "            if 'nccl' in line.lower():\n",
    "                print(f\"🔗 NCCL通信: {line.rstrip()}\")\n",
    "                last_output_time = current_time\n",
    "            \n",
    "            # 错误信息特别标记\n",
    "            if any(err in line.lower() for err in ['error', 'timeout', 'fail']):\n",
    "                print(f\"❌ 错误信息: {line.rstrip()}\")\n",
    "                last_output_time = current_time\n",
    "            \n",
    "            # 长时间无输出的警告\n",
    "            if current_time - last_output_time > 300:  # 5分钟无输出\n",
    "                elapsed_no_output = current_time - last_output_time\n",
    "                if dataset_preparation_detected:\n",
    "                    print(f\"\\n⏳ [数据集准备] 已有 {elapsed_no_output:.1f} 秒无输出，数据集构建中...\")\n",
    "                else:\n",
    "                    print(f\"\\n⏳ [等待中] 已有 {elapsed_no_output:.1f} 秒无输出...\")\n",
    "                last_output_time = current_time\n",
    "        \n",
    "        # 等待进程完成\n",
    "        return_code = process.wait()\n",
    "        \n",
    "        if return_code == 0:\n",
    "            print(f\"\\n✅ 训练成功完成! 总耗时: {time.time() - start_time:.1f} 秒\")\n",
    "        else:\n",
    "            print(f\"\\n❌ 训练失败，返回码: {return_code}\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n🛑 用户中断训练\")\n",
    "        process.terminate()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n💥 训练出错: {e}\")\n",
    "\n",
    "# 4. 启动训练\n",
    "print(\"🎯 启动带监控的训练...\")\n",
    "run_training_with_monitoring()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
