{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "concat_path = \"XTT22_train.fa\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_fasta_path = os.path.abspath(concat_path)\n",
    "output_dir = os.path.abspath(\"preprocessed_data\")\n",
    "output_yaml = f\"\"\"\n",
    "- datapaths: [\"{full_fasta_path}\"]\n",
    "  output_dir: \"{output_dir}\"\n",
    "  output_prefix: XTT22_train\n",
    "  train_split: 0.9\n",
    "  valid_split: 0.05\n",
    "  test_split: 0.05\n",
    "  overwrite: True\n",
    "  embed_reverse_complement: true\n",
    "  random_reverse_complement: 0.0\n",
    "  random_lineage_dropout: 0.0\n",
    "  include_sequence_id: false\n",
    "  transcribe: \"back_transcribe\"\n",
    "  force_uppercase: false\n",
    "  indexed_dataset_dtype: \"uint8\"\n",
    "  tokenizer_type: \"Byte-Level\"\n",
    "  vocab_file: null\n",
    "  vocab_size: null\n",
    "  merges_file: null\n",
    "  pretrained_tokenizer_model: null\n",
    "  special_tokens: null\n",
    "  fast_hf_tokenizer: true\n",
    "  append_eod: true\n",
    "  enforce_sample_length: null\n",
    "  ftfy: false\n",
    "  workers: 1\n",
    "  preproc_concurrency: 100000\n",
    "  chunksize: 25\n",
    "  drop_empty_sequences: true\n",
    "  nnn_filter: false  # If you split your fasta on NNN (in human these are contigs), then you should set this to true.\n",
    "  seed: 12342  # Not relevant because we are not using random reverse complement or lineage dropout.\n",
    "\"\"\"\n",
    "with open(\"preprocess_config.yaml\", \"w\") as f:\n",
    "    print(output_yaml, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!preprocess_evo2 --config preprocess_config.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh preprocessed_data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /workspace/hyena_modified.py /usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/model/hyena.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda129.so')\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/evo2_convert_to_nemo2\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/bionemo/evo2/utils/checkpoint/convert_to_nemo.py\", line 173, in main\n",
      "    importer.apply(args.output_dir)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/model/hyena.py\", line 520, in apply\n",
      "    source = self.get_source_model()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/model/hyena.py\", line 507, in get_source_model\n",
      "    return torch.load(str(self), map_location='cpu')\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1496, in load\n",
      "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
      "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL _io.BytesIO was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_io.BytesIO])` or the `torch.serialization.safe_globals([_io.BytesIO])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
     ]
    }
   ],
   "source": [
    "!evo2_convert_to_nemo2 \\\n",
    "  --model-path /workspace/savanna_evo2_7b/savanna_evo2_7b.pt \\\n",
    "  --model-size 7b --output-dir nemo2_evo2_7b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ å¤‡ä»½å¹¶æ›¿æ¢è®­ç»ƒè„šæœ¬...\n",
      "ğŸ”§ é…ç½®NCCLè¶…æ—¶å’Œä¼˜åŒ–å‚æ•°...\n",
      "ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ:\n",
      "  NCCL_TIMEOUT: 7200\n",
      "  TORCH_DISTRIBUTED_TIMEOUT: 7200\n",
      "  NCCL_DEBUG: INFO\n",
      "  NCCL_BUFFSIZE: 8388608\n",
      "ğŸ¯ å¯åŠ¨å¸¦ç›‘æ§çš„è®­ç»ƒ...\n",
      "ğŸ“ æ•°æ®é›†ç›®å½•: /workspace/preprocessed_data\n",
      "ğŸš€ å¼€å§‹è®­ç»ƒæ—¶é—´: 2025-06-05 12:03:51.918623\n",
      "ğŸ“‹ è®­ç»ƒå‘½ä»¤: train_evo2 -d training_data_config.yaml --dataset-dir /workspace/preprocessed_data --model-size 7b --devices 2 --num-nodes 1 --seq-length 1 --micro-batch-size 1 --lr 0.0001 --warmup-steps 5 --max-steps 200000 --ckpt-dir nemo2_evo2_7b --clip-grad 1 --wd 0.01 --activation-checkpoint-recompute-num-layers 1 --val-check-interval 1000 --ckpt-async-save\n",
      "================================================================================\n",
      "[12.2s] Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda129.so')\n",
      "[12.2s] The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[13.8s] ğŸ”§ è®¾ç½®PyTorchåˆ†å¸ƒå¼è¶…æ—¶: 7200ç§’\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Using byte-level tokenization\n",
      "[13.8s] [NeMo W 2025-06-05 12:04:05 nemo_logging:405] WandB is currently turned off.\n",
      "[13.8s] [NeMo W 2025-06-05 12:04:05 nemo_logging:405] User-set tensorboard is currently turned off. Internally one may still be set by NeMo2.\n",
      "[13.8s] Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "[13.8s] GPU available: True (cuda), used: True\n",
      "[13.8s] TPU available: False, using: 0 TPU cores\n",
      "[13.8s] HPU available: False, using: 0 HPUs\n",
      "[13.8s] å¯ç”¨ LoRA å¾®è°ƒ...\n",
      "[13.8s] æ¨¡å‹ç»“æ„è°ƒè¯•ä¿¡æ¯:\n",
      "[13.8s] --------------------------------------------------\n",
      "[13.8s] æ¨¡å‹æ€»å…±æœ‰ 1 ä¸ªæ¨¡å—\n",
      "[13.8s] å°è¯•è®¿é—®æ¨¡å‹çš„å…¶ä»–å±æ€§...\n",
      "[13.8s] æ¨¡å—ç»“æ„:\n",
      "[13.8s]    1.  (HyenaModel)\n",
      "[13.8s] \n",
      "[13.8s] ç›®æ ‡æ¨¡å¼: ['module.decoder.layers.17.self_attention.linear_qkv']\n",
      "[13.8s] \n",
      "[13.8s] æ‰€æœ‰çº¿æ€§å±‚ (0):\n",
      "[13.8s] \n",
      "[13.8s] æ‰¾åˆ°çš„æ³¨æ„åŠ›ç›¸å…³å±‚ (0):\n",
      "[13.8s] \n",
      "[13.8s] æ‰¾åˆ°çš„çº¿æ€§å±‚ (0):\n",
      "[13.8s] \n",
      "[13.8s] æ‰¾åˆ°çš„QKVå±‚ (0):\n",
      "[13.8s] âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•çº¿æ€§å±‚ï¼æ¨¡å‹å¯èƒ½è¿˜æ²¡æœ‰å®Œå…¨åˆå§‹åŒ–ã€‚\n",
      "[13.8s] è¿™åœ¨ NeMo/Megatron æ¡†æ¶ä¸­æ˜¯æ­£å¸¸çš„ï¼Œæ¨¡å‹ç»“æ„ä¼šåœ¨è®­ç»ƒå¼€å§‹æ—¶åˆå§‹åŒ–ã€‚\n",
      "[13.8s] LoRA é…ç½®å·²ä¿å­˜ï¼Œå°†åœ¨æ¨¡å‹å®Œå…¨åˆå§‹åŒ–ååº”ç”¨ã€‚\n",
      "[13.8s] æ€»å‚æ•°æ•°é‡: 0\n",
      "[13.8s] å¯è®­ç»ƒå‚æ•°æ•°é‡: 0\n",
      "[13.8s] âš ï¸  è­¦å‘Š: æ¨¡å‹æ€»å‚æ•°æ•°é‡ä¸º0ï¼Œå¯èƒ½æ¨¡å‹åˆå§‹åŒ–æœ‰é—®é¢˜\n",
      "[13.8s] âš ï¸  è­¦å‘Š: æ²¡æœ‰å¯è®­ç»ƒçš„å‚æ•°ï¼ŒLoRA å¯èƒ½æ²¡æœ‰æ­£ç¡®åº”ç”¨\n",
      "[13.8s] LoRA å°†åœ¨è®­ç»ƒå¼€å§‹æ—¶è‡ªåŠ¨åº”ç”¨\n",
      "[13.8s] å·²æ·»åŠ  LoRA å›è°ƒï¼Œå°†åœ¨è®­ç»ƒå¼€å§‹æ—¶åº”ç”¨ LoRA\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Experiments will be logged at results/evo2/dev\n",
      "[13.8s] [NeMo W 2025-06-05 12:04:05 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :results/evo2/checkpoints. Training from scratch.\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has data parallel group : [0, 1]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0, 1]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1]]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] All context parallel group ranks: [[0], [1]]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] All model parallel group ranks: [[0], [1]]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] All tensor model parallel group ranks: [[0], [1]]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1]]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] All embedding group ranks: [[0], [1]]\n",
      "[13.8s] [NeMo I 2025-06-05 12:04:05 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "[25.6s] Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda129.so')\n",
      "[25.6s] The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[27.5s] [W605 12:04:19.035675933 Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n",
      "ğŸ”— NCCLé€šä¿¡: [W605 12:04:19.035675933 Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n",
      "[27.5s] [W605 12:04:19.035692497 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "ğŸ”— NCCLé€šä¿¡: [W605 12:04:19.035692497 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "âŒ é”™è¯¯ä¿¡æ¯: [W605 12:04:19.035692497 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "[27.5s] [W605 12:04:19.046396689 Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n",
      "ğŸ”— NCCLé€šä¿¡: [W605 12:04:19.046396689 Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n",
      "[27.5s] [W605 12:04:19.046433724 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "ğŸ”— NCCLé€šä¿¡: [W605 12:04:19.046433724 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "âŒ é”™è¯¯ä¿¡æ¯: [W605 12:04:19.046433724 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "[27.5s] ----------------------------------------------------------------------------------------------------\n",
      "[27.5s] distributed_backend=nccl\n",
      "ğŸ”— NCCLé€šä¿¡: distributed_backend=nccl\n",
      "[27.5s] All distributed processes registered. Starting with 2 processes\n",
      "[27.5s] ----------------------------------------------------------------------------------------------------\n",
      "[27.5s] \n",
      "[28.1s] ubuntu:2017:2017 [0] NCCL INFO Bootstrap: Using eth0:172.21.72.20<0>\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2017 [0] NCCL INFO Bootstrap: Using eth0:172.21.72.20<0>\n",
      "[28.1s] ubuntu:2017:2017 [0] NCCL INFO cudaDriverVersion 12090\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2017 [0] NCCL INFO cudaDriverVersion 12090\n",
      "[28.1s] ubuntu:2017:2017 [0] NCCL INFO NCCL version 2.26.3+cuda12.9\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2017 [0] NCCL INFO NCCL version 2.26.3+cuda12.9\n",
      "[28.1s] ubuntu:2017:2017 [0] NCCL INFO Comm config Blocking set to 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2017 [0] NCCL INFO Comm config Blocking set to 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v10 symbol.\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v10 symbol.\n",
      "âŒ é”™è¯¯ä¿¡æ¯: ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v10 symbol.\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v9 (v9)\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v9 (v9)\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.\n",
      "âŒ é”™è¯¯ä¿¡æ¯: ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Loaded collnet plugin SHARP (v9)\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO NET/Plugin: Loaded collnet plugin SHARP (v9)\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO P2P plugin v9 IBext_v9\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO P2P plugin v9 IBext_v9\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/IB : No device found.\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO NET/IB : No device found.\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/IB : No device found.\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO NET/IB : No device found.\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO NET/Socket : Using [0]eth0:172.21.72.20<0> [1]eth1:172.25.58.75<0> [2]nodelocaldns:169.254.25.10<0> [3]vxlan.calico:10.233.112.0<0> [4]cali829ebf906e6:fe80::ecee:eeff:feee:eeee%cali829ebf906e6<0> [5]cali9ffd33e0038:fe80::ecee:eeff:feee:eeee%cali9ffd33e0038<0> [6]cali210c644397a:fe80::ecee:eeff:feee:eeee%cali210c644397a<0> [7]caliafd05346b22:fe80::ecee:eeff:feee:eeee%caliafd05346b22<0> [8]calicd149c44f0f:fe80::ecee:eeff:feee:eeee%calicd149c44f0f<0> [9]cali62597977d84:fe80::ecee:eeff:feee:eeee%cali62597977d84<0> [10]calidee47cce1bb:fe80::ecee:eeff:feee:eeee%calidee47cce1bb<0> [11]cali2ad9843228b:fe80::ecee:eeff:feee:eeee%cali2ad9843228b<0> [12]calicb98c4ca0f1:fe80::ecee:eeff:feee:eeee%calicb98c4ca0f1<0> [13]cali9c537539f6f:fe80::ecee:eeff:feee:eeee%cali9c537539f6f<0> [14]cali3dd684f6336:fe80::ecee:eeff:feee:eeee%cali3dd684f6336<0> [15]calid294f721236:fe80::ecee:eeff:feee:eeee%calid294f721236<0>\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO NET/Socket : Using [0]eth0:172.21.72.20<0> [1]eth1:172.25.58.75<0> [2]nodelocaldns:169.254.25.10<0> [3]vxlan.calico:10.233.112.0<0> [4]cali829ebf906e6:fe80::ecee:eeff:feee:eeee%cali829ebf906e6<0> [5]cali9ffd33e0038:fe80::ecee:eeff:feee:eeee%cali9ffd33e0038<0> [6]cali210c644397a:fe80::ecee:eeff:feee:eeee%cali210c644397a<0> [7]caliafd05346b22:fe80::ecee:eeff:feee:eeee%caliafd05346b22<0> [8]calicd149c44f0f:fe80::ecee:eeff:feee:eeee%calicd149c44f0f<0> [9]cali62597977d84:fe80::ecee:eeff:feee:eeee%cali62597977d84<0> [10]calidee47cce1bb:fe80::ecee:eeff:feee:eeee%calidee47cce1bb<0> [11]cali2ad9843228b:fe80::ecee:eeff:feee:eeee%cali2ad9843228b<0> [12]calicb98c4ca0f1:fe80::ecee:eeff:feee:eeee%calicb98c4ca0f1<0> [13]cali9c537539f6f:fe80::ecee:eeff:feee:eeee%cali9c537539f6f<0> [14]cali3dd684f6336:fe80::ecee:eeff:feee:eeee%cali3dd684f6336<0> [15]calid294f721236:fe80::ecee:eeff:feee:eeee%calid294f721236<0>\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Using network Socket\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Using network Socket\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO ncclCommInitRankConfig comm 0x459f9440 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 27000 commId 0x8c56ad3ef59f2eb8 - Init START\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO ncclCommInitRankConfig comm 0x459f9440 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 27000 commId 0x8c56ad3ef59f2eb8 - Init START\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Bootstrap timings total 0.020981 (create 0.000034, send 0.016701, recv 0.003764, ring 0.000058, delay 0.000001)\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Bootstrap timings total 0.020981 (create 0.000034, send 0.016701, recv 0.003764, ring 0.000058, delay 0.000001)\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO comm 0x459f9440 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO comm 0x459f9440 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 00/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 00/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 01/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 01/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 02/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 02/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 03/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 03/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 04/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 04/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 05/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 05/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 06/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 06/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 07/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 07/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 08/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 08/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 09/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 09/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 10/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 10/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 11/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 11/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 12/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 12/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 13/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 13/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 14/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 14/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 15/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 15/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 16/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 16/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 17/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 17/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 18/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 18/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 19/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 19/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 20/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 20/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 21/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 21/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 22/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 22/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Channel 23/24 : 0 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Channel 23/24 : 0 1\n",
      "[28.1s] ubuntu:2017:2226 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] -1/-1/-1->0->1 [9] -1/-1/-1->0->1 ubuntu:2110:2110 [1] NCCL INFO cudaDriverVersion 12090\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2017:2226 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] -1/-1/-1->0->1 [9] -1/-1/-1->0->1 ubuntu:2110:2110 [1] NCCL INFO cudaDriverVersion 12090\n",
      "[28.1s] ubuntu:2110:2110 [1] NCCL INFO Bootstrap: Using eth0:172.21.72.20<0>\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2110 [1] NCCL INFO Bootstrap: Using eth0:172.21.72.20<0>\n",
      "[28.1s] ubuntu:2110:2110 [1] NCCL INFO NCCL version 2.26.3+cuda12.9\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2110 [1] NCCL INFO NCCL version 2.26.3+cuda12.9\n",
      "[28.1s] ubuntu:2110:2110 [1] NCCL INFO Comm config Blocking set to 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2110 [1] NCCL INFO Comm config Blocking set to 1\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v10 symbol.\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v10 symbol.\n",
      "âŒ é”™è¯¯ä¿¡æ¯: ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v10 symbol.\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v9 (v9)\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v9 (v9)\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.\n",
      "âŒ é”™è¯¯ä¿¡æ¯: ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Loaded collnet plugin SHARP (v9)\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO NET/Plugin: Loaded collnet plugin SHARP (v9)\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO P2P plugin v9 IBext_v9\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO P2P plugin v9 IBext_v9\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/IB : No device found.\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO NET/IB : No device found.\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/IB : No device found.\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO NET/IB : No device found.\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.72.20<0>\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NET/Socket : Using [0]eth0:172.21.72.20<0> [1]eth1:172.25.58.75<0> [2]nodelocaldns:169.254.25.10<0> [3]vxlan.calico:10.233.112.0<0> [4]cali829ebf906e6:fe80::ecee:eeff:feee:eeee%cali829ebf906e6<0> [5]cali9ffd33e0038:fe80::ecee:eeff:feee:eeee%cali9ffd33e0038<0> [6]cali210c644397a:fe80::ecee:eeff:feee:eeee%cali210c644397a<0> [7]caliafd05346b22:fe80::ecee:eeff:feee:eeee%caliafd05346b22<0> [8]calicd149c44f0f:fe80::ecee:eeff:feee:eeee%calicd149c44f0f<0> [9]cali62597977d84:fe80::ecee:eeff:feee:eeee%cali62597977d84<0> [10]calidee47cce1bb:fe80::ecee:eeff:feee:eeee%calidee47cce1bb<0> [11]cali2ad9843228b:fe80::ecee:eeff:feee:eeee%cali2ad9843228b<0> [12]calicb98c4ca0f1:fe80::ecee:eeff:feee:eeee%calicb98c4ca0f1<0> [13]cali9c537539f6f:fe80::ecee:eeff:feee:eeee%cali9c537539f6f<0> [14]cali3dd684f6336:fe80::ecee:eeff:feee:eeee%cali3dd684f6336<0> [15]calid294f721236:fe80::ecee:eeff:feee:eeee%calid294f721236<0>\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO NET/Socket : Using [0]eth0:172.21.72.20<0> [1]eth1:172.25.58.75<0> [2]nodelocaldns:169.254.25.10<0> [3]vxlan.calico:10.233.112.0<0> [4]cali829ebf906e6:fe80::ecee:eeff:feee:eeee%cali829ebf906e6<0> [5]cali9ffd33e0038:fe80::ecee:eeff:feee:eeee%cali9ffd33e0038<0> [6]cali210c644397a:fe80::ecee:eeff:feee:eeee%cali210c644397a<0> [7]caliafd05346b22:fe80::ecee:eeff:feee:eeee%caliafd05346b22<0> [8]calicd149c44f0f:fe80::ecee:eeff:feee:eeee%calicd149c44f0f<0> [9]cali62597977d84:fe80::ecee:eeff:feee:eeee%cali62597977d84<0> [10]calidee47cce1bb:fe80::ecee:eeff:feee:eeee%calidee47cce1bb<0> [11]cali2ad9843228b:fe80::ecee:eeff:feee:eeee%cali2ad9843228b<0> [12]calicb98c4ca0f1:fe80::ecee:eeff:feee:eeee%calicb98c4ca0f1<0> [13]cali9c537539f6f:fe80::ecee:eeff:feee:eeee%cali9c537539f6f<0> [14]cali3dd684f6336:fe80::ecee:eeff:feee:eeee%cali3dd684f6336<0> [15]calid294f721236:fe80::ecee:eeff:feee:eeee%calid294f721236<0>\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO Using network Socket\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO Using network Socket\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO ncclCommInitRankConfig comm 0x22d88b30 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 2a000 commId 0x8c56ad3ef59f2eb8 - Init START\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO ncclCommInitRankConfig comm 0x22d88b30 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 2a000 commId 0x8c56ad3ef59f2eb8 - Init START\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO RAS client listening socket at 127.0.0.1<28028>\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO RAS client listening socket at 127.0.0.1<28028>\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO Bootstrap timings total 0.000840 (create 0.000033, send 0.000181, recv 0.000228, ring 0.000040, delay 0.000001)\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO Bootstrap timings total 0.000840 (create 0.000033, send 0.000181, recv 0.000228, ring 0.000040, delay 0.000001)\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO comm 0x22d88b30 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO comm 0x22d88b30 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] 0/-1/-1->1->-1 [7] 0/-1/-1->1->-1 [8] 0/-1/-1->1->-1 [9] 0/-1/-1->1->-1 [10] 0/-1/-1->1->-1 [11] 0/-1/-1->1->-1 [12] -1/-1/-1->1->0 [13] -1/-1/-1->1->0 [14] -1/-1/-1->1->0 [15] -1/-1/-1->1->0 [16] -1/-1/-1->1->0 [17] -1/-1/-1->1->0 [18] 0/-1/-1->1->-1 [19] 0/-1/-1->1->-1 [20] 0/-1/-1->1->-1 [21] 0/-1/-1->1->-1 [22] 0/-1/-1->1->-1 [23] 0/-1/-1->1->-1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] 0/-1/-1->1->-1 [7] 0/-1/-1->1->-1 [8] 0/-1/-1->1->-1 [9] 0/-1/-1->1->-1 [10] 0/-1/-1->1->-1 [11] 0/-1/-1->1->-1 [12] -1/-1/-1->1->0 [13] -1/-1/-1->1->0 [14] -1/-1/-1->1->0 [15] -1/-1/-1->1->0 [16] -1/-1/-1->1->0 [17] -1/-1/-1->1->0 [18] 0/-1/-1->1->-1 [19] 0/-1/-1->1->-1 [20] 0/-1/-1->1->-1 [21] 0/-1/-1->1->-1 [22] 0/-1/-1->1->-1 [23] 0/-1/-1->1->-1\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NCCL_BUFFSIZE set by environment to 8388608.\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO NCCL_BUFFSIZE set by environment to 8388608.\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO P2P Chunksize set to 524288\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO P2P Chunksize set to 524288\n",
      "[28.1s] ubuntu:2110:2232 [1] NCCL INFO [Proxy Service] Device 1 CPU core 71\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2232 [1] NCCL INFO [Proxy Service] Device 1 CPU core 71\n",
      "[28.1s] ubuntu:2110:2233 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 25\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2233 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 25\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO NCCL_NTHREADS set by environment to 8.\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO NCCL_NTHREADS set by environment to 8.\n",
      "[28.1s] \n",
      "[28.1s] [2025-06-05 12:04:20] ubuntu:2110:2227 [1] graph/tuning.cc:19 NCCL WARN Invalid NCCL_NTHREADS 8 (must be a multiple of 32)\n",
      "ğŸ”— NCCLé€šä¿¡: [2025-06-05 12:04:20] ubuntu:2110:2227 [1] graph/tuning.cc:19 NCCL WARN Invalid NCCL_NTHREADS 8 (must be a multiple of 32)\n",
      "[28.1s] \n",
      "[28.1s] [2025-06-05 12:04:20] ubuntu:2110:2227 [1] graph/tuning.cc:19 NCCL WARN Invalid NCCL_NTHREADS 8 (must be a multiple of 32)\n",
      "ğŸ”— NCCLé€šä¿¡: [2025-06-05 12:04:20] ubuntu:2110:2227 [1] graph/tuning.cc:19 NCCL WARN Invalid NCCL_NTHREADS 8 (must be a multiple of 32)\n",
      "[28.1s] \n",
      "[28.1s] [2025-06-05 12:04:20] ubuntu:2110:2227 [1] graph/tuning.cc:19 NCCL WARN Invalid NCCL_NTHREADS 8 (must be a multiple of 32)\n",
      "ğŸ”— NCCLé€šä¿¡: [2025-06-05 12:04:20] ubuntu:2110:2227 [1] graph/tuning.cc:19 NCCL WARN Invalid NCCL_NTHREADS 8 (must be a multiple of 32)\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "[28.1s] ubuntu:2110:2227 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\n",
      "[28.5s] ubuntu:2110:2227 [1] [NeMo I 2025-06-05 12:04:20 utils:554] Building Evo2Dataset splits with sizes=[400000, 8040, 2] and config=GPTDatasetConfig(random_seed=1234, sequence_length=1, blend=None, blend_per_split=[(['/workspace/preprocessed_data/XTT22_train_byte-level_train'], [1.0]), (['/workspace/preprocessed_data/XTT22_train_byte-level_val'], [1.0]), (['/workspace/preprocessed_data/XTT22_train_byte-level_test'], [1.0])], split=None, split_matrix=None, num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.bytelevel_tokenizers.ByteLevelTokenizer object at 0x727accc3c1d0>, mid_level_dataset_surplus=0.005, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, object_storage_cache_path=None)\n",
      "ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: ubuntu:2110:2227 [1] [NeMo I 2025-06-05 12:04:20 utils:554] Building Evo2Dataset splits with sizes=[400000, 8040, 2] and config=GPTDatasetConfig(random_seed=1234, sequence_length=1, blend=None, blend_per_split=[(['/workspace/preprocessed_data/XTT22_train_byte-level_train'], [1.0]), (['/workspace/preprocessed_data/XTT22_train_byte-level_val'], [1.0]), (['/workspace/preprocessed_data/XTT22_train_byte-level_test'], [1.0])], split=None, split_matrix=None, num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.bytelevel_tokenizers.ByteLevelTokenizer object at 0x727accc3c1d0>, mid_level_dataset_surplus=0.005, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, object_storage_cache_path=None)\n",
      "[28.5s] [NeMo I 2025-06-05 12:04:20 utils:554] Load the _IndexReader from /workspace/preprocessed_data/XTT22_train_byte-level_train.idx\n",
      "ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: [NeMo I 2025-06-05 12:04:20 utils:554] Load the _IndexReader from /workspace/preprocessed_data/XTT22_train_byte-level_train.idx\n",
      "[28.5s] [NeMo I 2025-06-05 12:04:20 utils:554] \tExtract the sequence lengths\n",
      "[28.5s] [NeMo I 2025-06-05 12:04:20 utils:554] \tExtract the sequence pointers\n",
      "[28.5s] [NeMo I 2025-06-05 12:04:20 utils:554] \tExtract the document indices\n",
      "[28.5s] [NeMo I 2025-06-05 12:04:20 utils:554] > total number of sequences: 10896\n",
      "[28.5s] [NeMo I 2025-06-05 12:04:20 utils:554] > total number of documents: 10896\n",
      "[28.5s] [NeMo I 2025-06-05 12:04:20 utils:554] Build and save the Evo2Dataset train indices\n",
      "ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: [NeMo I 2025-06-05 12:04:20 utils:554] Build and save the Evo2Dataset train indices\n",
      "[628.5s] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.\n",
      "ğŸ”— NCCLé€šä¿¡: NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.\n",
      "[628.5s] ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.\n",
      "âŒ é”™è¯¯ä¿¡æ¯: ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.\n",
      "[628.5s] ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.\n",
      "âŒ é”™è¯¯ä¿¡æ¯: ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.\n",
      "[628.5s] ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.\n",
      "âŒ é”™è¯¯ä¿¡æ¯: ubuntu:2110:2227 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.\n",
      "[628.5s] ubuntu:2110:2227 [1] NCCL INFO ncclCommInitRankConfig comm 0x22d88b30 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 2a000 commId 0x8c56ad3ef59f2eb8 - Init COMPLETE\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO ncclCommInitRankConfig comm 0x22d88b30 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 2a000 commId 0x8c56ad3ef59f2eb8 - Init COMPLETE\n",
      "[628.5s] ubuntu:2110:2227 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.26 (kernels 0.13, alloc 0.09, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.01, rest 0.01)\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2227 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.26 (kernels 0.13, alloc 0.09, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.01, rest 0.01)\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[628.5s] ubuntu:2110:2235 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2235 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1\n",
      "[628.5s] [rank1]:[E605 12:14:20.023183723 ProcessGroupNCCL.cpp:633] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n",
      "ğŸ”— NCCLé€šä¿¡: [rank1]:[E605 12:14:20.023183723 ProcessGroupNCCL.cpp:633] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n",
      "âŒ é”™è¯¯ä¿¡æ¯: [rank1]:[E605 12:14:20.023183723 ProcessGroupNCCL.cpp:633] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n",
      "[628.5s] [rank1]:[E605 12:14:20.023544412 ProcessGroupNCCL.cpp:757] [Rank 1] Work WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) timed out in blocking wait.\n",
      "ğŸ”— NCCLé€šä¿¡: [rank1]:[E605 12:14:20.023544412 ProcessGroupNCCL.cpp:757] [Rank 1] Work WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) timed out in blocking wait.\n",
      "âŒ é”™è¯¯ä¿¡æ¯: [rank1]:[E605 12:14:20.023544412 ProcessGroupNCCL.cpp:757] [Rank 1] Work WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) timed out in blocking wait.\n",
      "[629.0s] ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:64 -> 3\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:64 -> 3\n",
      "[629.0s] ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:80 -> 3\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:80 -> 3\n",
      "[629.0s] ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:829 -> 3\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:829 -> 3\n",
      "[629.0s] ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:64 -> 3\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:64 -> 3\n",
      "[629.0s] ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:80 -> 3\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:80 -> 3\n",
      "[629.0s] ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:829 -> 3\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2237 [1] NCCL INFO misc/socket.cc:829 -> 3\n",
      "[629.0s] ubuntu:2110:2232 [1] NCCL INFO misc/socket.cc:881 -> 3\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2232 [1] NCCL INFO misc/socket.cc:881 -> 3\n",
      "[629.0s] ubuntu:2110:2237 [1] NCCL INFO comm 0x22d88b30 rank 1 nranks 2 cudaDev 1 busId 2a000 - Abort COMPLETE\n",
      "ğŸ”— NCCLé€šä¿¡: ubuntu:2110:2237 [1] NCCL INFO comm 0x22d88b30 rank 1 nranks 2 cudaDev 1 busId 2a000 - Abort COMPLETE\n",
      "[629.0s] [rank1]:[E605 12:14:20.550697962 ProcessGroupNCCL.cpp:685] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n",
      "ğŸ”— NCCLé€šä¿¡: [rank1]:[E605 12:14:20.550697962 ProcessGroupNCCL.cpp:685] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n",
      "âŒ é”™è¯¯ä¿¡æ¯: [rank1]:[E605 12:14:20.550697962 ProcessGroupNCCL.cpp:685] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n",
      "[629.0s] [rank1]:[E605 12:14:20.550744322 ProcessGroupNCCL.cpp:699] [Rank 1] To avoid data inconsistency, we are taking the entire process down.\n",
      "ğŸ”— NCCLé€šä¿¡: [rank1]:[E605 12:14:20.550744322 ProcessGroupNCCL.cpp:699] [Rank 1] To avoid data inconsistency, we are taking the entire process down.\n",
      "[629.0s] ğŸ”§ è®¾ç½®PyTorchåˆ†å¸ƒå¼è¶…æ—¶: 7200ç§’\n",
      "[629.0s] å¯ç”¨ LoRA å¾®è°ƒ...\n",
      "[629.0s] æ¨¡å‹ç»“æ„è°ƒè¯•ä¿¡æ¯:\n",
      "[629.0s] --------------------------------------------------\n",
      "[629.0s] æ¨¡å‹æ€»å…±æœ‰ 1 ä¸ªæ¨¡å—\n",
      "[629.0s] å°è¯•è®¿é—®æ¨¡å‹çš„å…¶ä»–å±æ€§...\n",
      "[629.0s] æ¨¡å—ç»“æ„:\n",
      "[629.0s]    1.  (HyenaModel)\n",
      "[629.0s] \n",
      "[629.0s] ç›®æ ‡æ¨¡å¼: ['module.decoder.layers.17.self_attention.linear_qkv']\n",
      "[629.0s] \n",
      "[629.0s] æ‰€æœ‰çº¿æ€§å±‚ (0):\n",
      "[629.0s] \n",
      "[629.0s] æ‰¾åˆ°çš„æ³¨æ„åŠ›ç›¸å…³å±‚ (0):\n",
      "[629.0s] \n",
      "[629.0s] æ‰¾åˆ°çš„çº¿æ€§å±‚ (0):\n",
      "[629.0s] \n",
      "[629.0s] æ‰¾åˆ°çš„QKVå±‚ (0):\n",
      "[629.0s] âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•çº¿æ€§å±‚ï¼æ¨¡å‹å¯èƒ½è¿˜æ²¡æœ‰å®Œå…¨åˆå§‹åŒ–ã€‚\n",
      "[629.0s] è¿™åœ¨ NeMo/Megatron æ¡†æ¶ä¸­æ˜¯æ­£å¸¸çš„ï¼Œæ¨¡å‹ç»“æ„ä¼šåœ¨è®­ç»ƒå¼€å§‹æ—¶åˆå§‹åŒ–ã€‚\n",
      "[629.0s] LoRA é…ç½®å·²ä¿å­˜ï¼Œå°†åœ¨æ¨¡å‹å®Œå…¨åˆå§‹åŒ–ååº”ç”¨ã€‚\n",
      "[629.0s] æ€»å‚æ•°æ•°é‡: 0\n",
      "[629.0s] å¯è®­ç»ƒå‚æ•°æ•°é‡: 0\n",
      "[629.0s] âš ï¸  è­¦å‘Š: æ¨¡å‹æ€»å‚æ•°æ•°é‡ä¸º0ï¼Œå¯èƒ½æ¨¡å‹åˆå§‹åŒ–æœ‰é—®é¢˜\n",
      "[629.0s] âš ï¸  è­¦å‘Š: æ²¡æœ‰å¯è®­ç»ƒçš„å‚æ•°ï¼ŒLoRA å¯èƒ½æ²¡æœ‰æ­£ç¡®åº”ç”¨\n",
      "[629.0s] LoRA å°†åœ¨è®­ç»ƒå¼€å§‹æ—¶è‡ªåŠ¨åº”ç”¨\n",
      "[629.0s] å·²æ·»åŠ  LoRA å›è°ƒï¼Œå°†åœ¨è®­ç»ƒå¼€å§‹æ—¶åº”ç”¨ LoRA\n",
      "[629.0s] [rank1]: Traceback (most recent call last):\n",
      "[629.0s] [rank1]:   File \"/usr/local/bin/train_evo2\", line 10, in <module>\n",
      "[629.0s] [rank1]:     sys.exit(main())\n",
      "[629.0s] [rank1]:              ^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py\", line 1133, in main\n",
      "[629.0s] [rank1]:     train(args=args)\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py\", line 1126, in train\n",
      "[629.0s] [rank1]:     trainer.fit(model, data_module)\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\n",
      "[629.0s] [rank1]:     call._call_and_handle_interrupt(\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
      "[629.0s] [rank1]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "[629.0s] [rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
      "[629.0s] [rank1]:     return function(*args, **kwargs)\n",
      "[629.0s] [rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\n",
      "[629.0s] [rank1]:     self._run(model, ckpt_path=ckpt_path)\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\", line 943, in _run\n",
      "[629.0s] [rank1]:     call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment\n",
      "[629.0s] [rank1]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\", line 102, in _call_setup_hook\n",
      "[629.0s] [rank1]:     _call_lightning_datamodule_hook(trainer, \"setup\", stage=fn)\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\", line 189, in _call_lightning_datamodule_hook\n",
      "[629.0s] [rank1]:     return fn(*args, **kwargs)\n",
      "[629.0s] [rank1]:            ^^^^^^^^^^^^^^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/data/pre_training.py\", line 314, in setup\n",
      "[629.0s] [rank1]:     self.build(\n",
      "ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: [rank1]:     self.build(\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/data/pre_training.py\", line 304, in build\n",
      "ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/gpt/data/pre_training.py\", line 304, in build\n",
      "[629.0s] [rank1]:     ).build()\n",
      "ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: [rank1]:     ).build()\n",
      "[629.0s] [rank1]:       ^^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 132, in build\n",
      "ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 132, in build\n",
      "[629.0s] [rank1]:     datasets = self._build_blended_dataset_splits()\n",
      "ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: [rank1]:     datasets = self._build_blended_dataset_splits()\n",
      "[629.0s] [rank1]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 253, in _build_blended_dataset_splits\n",
      "ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 253, in _build_blended_dataset_splits\n",
      "[629.0s] [rank1]:     blended_datasets[i] = self._build_megatron_dataset_splits(\n",
      "ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: [rank1]:     blended_datasets[i] = self._build_megatron_dataset_splits(\n",
      "[629.0s] [rank1]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 443, in _build_megatron_dataset_splits\n",
      "ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 443, in _build_megatron_dataset_splits\n",
      "[629.0s] [rank1]:     self.build_generic_dataset(\n",
      "ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: [rank1]:     self.build_generic_dataset(\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 507, in build_generic_dataset\n",
      "ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/megatron/core/datasets/blended_megatron_dataset_builder.py\", line 507, in build_generic_dataset\n",
      "[629.0s] [rank1]:     torch.distributed.barrier()\n",
      "ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: [rank1]:     torch.distributed.barrier()\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
      "[629.0s] [rank1]:     return func(*args, **kwargs)\n",
      "[629.0s] [rank1]:            ^^^^^^^^^^^^^^^^^^^^^\n",
      "[629.0s] [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 4662, in barrier\n",
      "ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: [rank1]:   File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 4662, in barrier\n",
      "[629.0s] [rank1]:     work.wait()\n",
      "[629.0s] [rank1]: torch.distributed.DistBackendError: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n",
      "ğŸ”— NCCLé€šä¿¡: [rank1]: torch.distributed.DistBackendError: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n",
      "âŒ é”™è¯¯ä¿¡æ¯: [rank1]: torch.distributed.DistBackendError: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n",
      "[634.3s] [rank: 1] Child process with PID 2110 terminated with code 1. Forcefully terminating all other processes to avoid zombies ğŸ§Ÿ\n",
      "\n",
      "âŒ è®­ç»ƒå¤±è´¥ï¼Œè¿”å›ç : -9\n"
     ]
    }
   ],
   "source": [
    "# ==================== NCCLè¶…æ—¶é—®é¢˜è§£å†³æ–¹æ¡ˆ ====================\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. å¤‡ä»½å’Œæ›¿æ¢è®­ç»ƒè„šæœ¬\n",
    "print(\"ğŸ”§ å¤‡ä»½å¹¶æ›¿æ¢è®­ç»ƒè„šæœ¬...\")\n",
    "!cp /usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py /usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py.backup 2>/dev/null || echo \"å¤‡ä»½æ–‡ä»¶å·²å­˜åœ¨æˆ–è·¯å¾„ä¸å­˜åœ¨\"\n",
    "!cp /workspace/bionemo_train.py /usr/local/lib/python3.12/dist-packages/bionemo/evo2/run/train.py 2>/dev/null || echo \"è‡ªå®šä¹‰è®­ç»ƒè„šæœ¬ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤ç‰ˆæœ¬\"\n",
    "\n",
    "# 2. è®¾ç½®NCCLå’Œåˆ†å¸ƒå¼ç¯å¢ƒå˜é‡\n",
    "print(\"ğŸ”§ é…ç½®NCCLè¶…æ—¶å’Œä¼˜åŒ–å‚æ•°...\")\n",
    "\n",
    "# NCCLè¶…æ—¶è®¾ç½® - å¢åŠ åˆ°2å°æ—¶\n",
    "os.environ['NCCL_TIMEOUT'] = '7200'  # 2å°æ—¶è¶…æ—¶\n",
    "os.environ['TORCH_NCCL_BLOCKING_WAIT'] = '1'  # ä½¿ç”¨æ–°çš„ç¯å¢ƒå˜é‡å\n",
    "os.environ['TORCH_NCCL_ASYNC_ERROR_HANDLING'] = '1'  # ä½¿ç”¨æ–°çš„ç¯å¢ƒå˜é‡å\n",
    "os.environ['NCCL_DEBUG'] = 'INFO'  # å¯ç”¨è¯¦ç»†è°ƒè¯•ä¿¡æ¯\n",
    "\n",
    "# PyTorchåˆ†å¸ƒå¼è¶…æ—¶è®¾ç½®\n",
    "os.environ['TORCH_DISTRIBUTED_TIMEOUT'] = '7200'  # PyTorchåˆ†å¸ƒå¼è¶…æ—¶\n",
    "os.environ['TORCH_NCCL_TRACE_BUFFER_SIZE'] = '1024'  # å¯ç”¨NCCLè·Ÿè¸ª\n",
    "\n",
    "# æ•°æ®åŠ è½½å’Œé€šä¿¡ä¼˜åŒ–\n",
    "os.environ['NCCL_BUFFSIZE'] = '8388608'  # å¢åŠ ç¼“å†²åŒºå¤§å°åˆ°8MB\n",
    "os.environ['NCCL_NTHREADS'] = '8'  # å¢åŠ NCCLçº¿ç¨‹æ•°\n",
    "os.environ['NCCL_MIN_NTHREADS'] = '4'  # æœ€å°çº¿ç¨‹æ•°\n",
    "\n",
    "# é¿å…å†…å­˜ç¢ç‰‡å’Œå¹¶è¡Œå†²çª\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # é¿å…tokenizerå¹¶è¡Œå†²çª\n",
    "os.environ['OMP_NUM_THREADS'] = '4'  # é™åˆ¶OpenMPçº¿ç¨‹æ•°\n",
    "\n",
    "# æ•°æ®é›†å‡†å¤‡ä¼˜åŒ–\n",
    "os.environ['NCCL_P2P_DISABLE'] = '0'  # ç¡®ä¿P2Pé€šä¿¡å¯ç”¨\n",
    "os.environ['NCCL_SHM_DISABLE'] = '0'  # ç¡®ä¿å…±äº«å†…å­˜é€šä¿¡å¯ç”¨\n",
    "\n",
    "print(\"ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ:\")\n",
    "for key in ['NCCL_TIMEOUT', 'TORCH_DISTRIBUTED_TIMEOUT', 'NCCL_DEBUG', 'NCCL_BUFFSIZE']:\n",
    "    print(f\"  {key}: {os.environ.get(key)}\")\n",
    "\n",
    "# 3. å®šä¹‰å¸¦ç›‘æ§çš„è®­ç»ƒå‡½æ•°\n",
    "def run_training_with_monitoring():\n",
    "    \"\"\"å¸¦å®æ—¶ç›‘æ§çš„è®­ç»ƒå¯åŠ¨å‡½æ•°\"\"\"\n",
    "    \n",
    "    # è·å–å½“å‰å·¥ä½œç›®å½•ä¸­çš„preprocessed_dataè·¯å¾„\n",
    "    preprocessed_data = os.path.abspath(\"preprocessed_data\")\n",
    "    print(f\"ğŸ“ æ•°æ®é›†ç›®å½•: {preprocessed_data}\")\n",
    "    \n",
    "    # è®­ç»ƒé…ç½®å‚æ•°\n",
    "    training_config = {\n",
    "        'data_config': 'training_data_config.yaml',\n",
    "        'dataset_dir': preprocessed_data,  # ä½¿ç”¨å®é™…è·¯å¾„\n",
    "        'model_size': '7b',\n",
    "        'devices': 2,\n",
    "        'num_nodes': 1,\n",
    "        'seq_length': 1,\n",
    "        'micro_batch_size': 1,\n",
    "        'lr': 0.0001,\n",
    "        'warmup_steps': 5,\n",
    "        'max_steps': 200000,\n",
    "        'ckpt_dir': 'nemo2_evo2_7b',\n",
    "        'clip_grad': 1,\n",
    "        'wd': 0.01,\n",
    "        'activation_checkpoint_recompute_num_layers': 1,\n",
    "        'val_check_interval': 1000\n",
    "    }\n",
    "    \n",
    "    # æ„å»ºè®­ç»ƒå‘½ä»¤ - ä½¿ç”¨æ­£ç¡®çš„æ ¼å¼\n",
    "    cmd_parts = [\n",
    "        'train_evo2',\n",
    "        '-d', training_config['data_config'],\n",
    "        '--dataset-dir', training_config['dataset_dir'],\n",
    "        '--model-size', training_config['model_size'],\n",
    "        '--devices', str(training_config['devices']),\n",
    "        '--num-nodes', str(training_config['num_nodes']),\n",
    "        '--seq-length', str(training_config['seq_length']),\n",
    "        '--micro-batch-size', str(training_config['micro_batch_size']),\n",
    "        '--lr', str(training_config['lr']),\n",
    "        '--warmup-steps', str(training_config['warmup_steps']),\n",
    "        '--max-steps', str(training_config['max_steps']),\n",
    "        '--ckpt-dir', training_config['ckpt_dir'],\n",
    "        '--clip-grad', str(training_config['clip_grad']),\n",
    "        '--wd', str(training_config['wd']),\n",
    "        '--activation-checkpoint-recompute-num-layers', str(training_config['activation_checkpoint_recompute_num_layers']),\n",
    "        '--val-check-interval', str(training_config['val_check_interval']),\n",
    "        '--ckpt-async-save'\n",
    "    ]\n",
    "    \n",
    "    cmd = ' '.join(cmd_parts)\n",
    "    \n",
    "    print(f\"ğŸš€ å¼€å§‹è®­ç»ƒæ—¶é—´: {datetime.now()}\")\n",
    "    print(f\"ğŸ“‹ è®­ç»ƒå‘½ä»¤: {cmd}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # å¯åŠ¨è®­ç»ƒè¿›ç¨‹\n",
    "        process = subprocess.Popen(\n",
    "            cmd_parts,  # ä½¿ç”¨åˆ—è¡¨è€Œä¸æ˜¯å­—ç¬¦ä¸²ï¼Œé¿å…shellè§£æé—®é¢˜\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            universal_newlines=True,\n",
    "            bufsize=1\n",
    "        )\n",
    "        \n",
    "        # å®æ—¶ç›‘æ§è¾“å‡º\n",
    "        start_time = time.time()\n",
    "        last_output_time = start_time\n",
    "        dataset_preparation_detected = False\n",
    "        \n",
    "        for line in iter(process.stdout.readline, ''):\n",
    "            current_time = time.time()\n",
    "            elapsed = current_time - start_time\n",
    "            \n",
    "            # æ‰“å°å¸¦æ—¶é—´æˆ³çš„è¾“å‡º\n",
    "            print(f\"[{elapsed:.1f}s] {line.rstrip()}\")\n",
    "            \n",
    "            # æ£€æŸ¥å…³é”®ä¿¡æ¯\n",
    "            keywords = ['dataset', 'preparing', 'loading', 'barrier', 'build', 'index']\n",
    "            if any(keyword in line.lower() for keyword in keywords):\n",
    "                print(f\"ğŸ“Š æ•°æ®é›†å‡†å¤‡é˜¶æ®µ: {line.rstrip()}\")\n",
    "                dataset_preparation_detected = True\n",
    "                last_output_time = current_time\n",
    "            \n",
    "            # NCCLç›¸å…³ä¿¡æ¯ç‰¹åˆ«æ ‡è®°\n",
    "            if 'nccl' in line.lower():\n",
    "                print(f\"ğŸ”— NCCLé€šä¿¡: {line.rstrip()}\")\n",
    "                last_output_time = current_time\n",
    "            \n",
    "            # é”™è¯¯ä¿¡æ¯ç‰¹åˆ«æ ‡è®°\n",
    "            if any(err in line.lower() for err in ['error', 'timeout', 'fail']):\n",
    "                print(f\"âŒ é”™è¯¯ä¿¡æ¯: {line.rstrip()}\")\n",
    "                last_output_time = current_time\n",
    "            \n",
    "            # é•¿æ—¶é—´æ— è¾“å‡ºçš„è­¦å‘Š\n",
    "            if current_time - last_output_time > 300:  # 5åˆ†é’Ÿæ— è¾“å‡º\n",
    "                elapsed_no_output = current_time - last_output_time\n",
    "                if dataset_preparation_detected:\n",
    "                    print(f\"\\nâ³ [æ•°æ®é›†å‡†å¤‡] å·²æœ‰ {elapsed_no_output:.1f} ç§’æ— è¾“å‡ºï¼Œæ•°æ®é›†æ„å»ºä¸­...\")\n",
    "                else:\n",
    "                    print(f\"\\nâ³ [ç­‰å¾…ä¸­] å·²æœ‰ {elapsed_no_output:.1f} ç§’æ— è¾“å‡º...\")\n",
    "                last_output_time = current_time\n",
    "        \n",
    "        # ç­‰å¾…è¿›ç¨‹å®Œæˆ\n",
    "        return_code = process.wait()\n",
    "        \n",
    "        if return_code == 0:\n",
    "            print(f\"\\nâœ… è®­ç»ƒæˆåŠŸå®Œæˆ! æ€»è€—æ—¶: {time.time() - start_time:.1f} ç§’\")\n",
    "        else:\n",
    "            print(f\"\\nâŒ è®­ç»ƒå¤±è´¥ï¼Œè¿”å›ç : {return_code}\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nğŸ›‘ ç”¨æˆ·ä¸­æ–­è®­ç»ƒ\")\n",
    "        process.terminate()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nğŸ’¥ è®­ç»ƒå‡ºé”™: {e}\")\n",
    "\n",
    "# 4. å¯åŠ¨è®­ç»ƒ\n",
    "print(\"ğŸ¯ å¯åŠ¨å¸¦ç›‘æ§çš„è®­ç»ƒ...\")\n",
    "run_training_with_monitoring()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
